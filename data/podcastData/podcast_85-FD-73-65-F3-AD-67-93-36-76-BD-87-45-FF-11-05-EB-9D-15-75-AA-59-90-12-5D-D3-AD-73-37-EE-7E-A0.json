{
    "Id": "85-FD-73-65-F3-AD-67-93-36-76-BD-87-45-FF-11-05-EB-9D-15-75-AA-59-90-12-5D-D3-AD-73-37-EE-7E-A0",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "OpenAI GPT-3: Language Models are Few-Shot Learners",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/OpenAI-GPT-3-Language-Models-are-Few-Shot-Learners-ef39ef",
    "Description": "<p>In this episode of Machine Learning Street Talk, Tim Scarfe, Yannic Kilcher and Connor Shorten discuss their takeaways from OpenAI\u2019s GPT-3 language model. With the help of Microsoft\u2019s ZeRO-2 / DeepSpeed optimiser, OpenAI trained an 175 BILLION parameter autoregressive language model. The paper demonstrates how self-supervised language modelling at this scale can perform many downstream tasks without fine-tuning.</p>\n<p><br></p>\n<p>00:00:00 Intro</p>\n<p>00:00:54 ZeRO1+2 (model + Data parallelism) (Connor)</p>\n<p>00:03:17 Recent history of NLP (Tim)</p>\n<p>00:06:04 Yannic \"Light-speed\" Kilcher's brief overview of GPT-3</p>\n<p>00:14:25 Reviewing Yannic's YT comments on his GPT-3 video (Tim)</p>\n<p>00:20:26 Main show intro</p>\n<p>00:23:03 Is GPT-3 reasoning?&nbsp;</p>\n<p>00:28:15 Architecture discussion and autoregressive (GPT*) vs denoising autoencoder (BERT)</p>\n<p>00:36:18 Utility of GPT-3 in industry</p>\n<p>00:43:03 Can GPT-3 do math? (reasoning/system 1/system 2)</p>\n<p>00:51:03 Generalisation</p>\n<p>00:56:48 Esoterics of language models</p>\n<p>00:58:46 Architectural trade-offs</p>\n<p>01:07:37 Memorization machines and intepretability</p>\n<p>01:17:16 Nearest neighbour probes / watermarks</p>\n<p>01:20:03 YouTube comments on GPT-3 video&nbsp;</p>\n<p>01:21:50 GPT-3 news article generation issue</p>\n<p>01:27:36 Sampling data for language models / bias / fairness / politics</p>\n<p>01:51:12 Outro</p>\n<p><br></p>\n<p>These paradigms of task adaptation are divided into zero, one, and few shot learning. Zero-shot learning is a very extreme case where we expect a language model to perform a task such as sentiment classification or extractive question answering, without any additional supervision. One and Few-shot learning provide some examples to the model. However, GPT-3s definition of this diverges a bit from the conventional literature. GPT-3 provides one and few-shot examples in the form of \u201cIn-Context Learning\u201d. Instead of fine-tuning the model on a few examples, the model has to use the input to infer the downstream task. For example, the GPT-3 transformer has an input sequence of 2048 tokens, so demonstrations of a task such as yelp sentiment reviews, would have to fit in this input sequence as well as the new review.</p>\n<p><br></p>\n<p>Thanks for watching! Please Subscribe!</p>\n<p>Paper Links:</p>\n<p>GPT-3: https://arxiv.org/abs/2005.14165</p>\n<p>ZeRO: https://arxiv.org/abs/1910.02054</p>\n<p>ZeRO (Blog Post): https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/</p>\n<p>ZeRO-2 (Blog Post): https://www.microsoft.com/en-us/research/blog/zero-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/?OCID=msr_blog_deepspeed2_build_tw</p>\n<p><br></p>\n<p>#machinelearning #naturallanguageprocessing #deeplearning #gpt3</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/14836623/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-5-6%2F80107678-44100-2-e8d0859bdb673.mp3"
}