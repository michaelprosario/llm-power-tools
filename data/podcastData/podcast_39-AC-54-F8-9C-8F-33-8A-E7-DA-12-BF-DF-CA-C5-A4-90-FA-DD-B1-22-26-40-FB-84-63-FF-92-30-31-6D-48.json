{
    "Id": "39-AC-54-F8-9C-8F-33-8A-E7-DA-12-BF-DF-CA-C5-A4-90-FA-DD-B1-22-26-40-FB-84-63-FF-92-30-31-6D-48",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "#74 Dr. ANDREW LAMPINEN - Symbolic behaviour in AI [UNPLUGGED]",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/74-Dr--ANDREW-LAMPINEN---Symbolic-behaviour-in-AI-UNPLUGGED-e1h6far",
    "Description": "<p>Please note that in this interview Dr. Lampinen was expressing his personal opinions and they do not necessarily represent those of DeepMind.&nbsp;</p>\n<p><br></p>\n<p>Patreon: https://www.patreon.com/mlst</p>\n<p>Discord: https://discord.gg/ESrGqhf5CB</p>\n<p>YT version: https://youtu.be/yPMtSXXn4OY</p>\n<p><br></p>\n<p>&nbsp;Dr. Andrew Lampinen is a Senior Research Scientist at DeepMind, and he thinks that symbols are subjective in the relativistic sense. Dr. Lampinen completed his PhD in Cognitive Psychology at Stanford University. His background is in mathematics, physics, and machine learning. Andrew has said that his research interests are in cognitive flexibililty and generalization, and how these abilities are enabled by factors like language, memory, and embodiment. &nbsp;Andrew with his coauthors has just released a paper called symbolic behaviour in artificial intelligence. Andrew lead in the paper by saying the human ability to use symbols has yet to be replicated in machines. He thinks that one of the key areas to bridge the gap here is considering how symbol meaning is established, and he strongly believes it is the symbol users themselves who agree upon the symbol meaning, And that the use of symbols entails behaviours which coalesce agreements about their meaning. Which in plain English means that symbols are defined by behaviours rather than their content.</p>\n<p><br></p>\n<p>[00:00:00] Intro to Andrew and Symbolic Behaviour paper</p>\n<p>[00:07:01] Semantics underpins the unreasonable effectiveness of symbols</p>\n<p>[00:12:56] The Depth of Subjectivity</p>\n<p>[00:21:03] Walid Saba - universal cognitive templates</p>\n<p>[00:27:47] Insufficiently Darwinian&nbsp;</p>\n<p>[00:30:52] Discovered vs invented</p>\n<p>[00:34:19] Does language have primacy</p>\n<p>[00:35:59] Research directions</p>\n<p>[00:39:43] Comparison to BenG OpenCog and human compatible AI</p>\n<p>[00:42:53] Aligning AI with our culture</p>\n<p>[00:47:55] Do we need to model the worst aspects of human behaviour?&nbsp;</p>\n<p>[00:50:57] Fairness</p>\n<p>[00:54:24] Memorisatation on LLMs</p>\n<p>[01:00:38] Wason selection task</p>\n<p>[01:03:45] Would an Andrew hashtable robot be intelligent?</p>\n<p><br></p>\n<p>Dr. Andrew Lampinen</p>\n<p>https://lampinen.github.io/</p>\n<p>https://twitter.com/AndrewLampinen</p>\n<p><br></p>\n<p>Symbolic Behaviour in Artificial Intelligence</p>\n<p>https://arxiv.org/abs/2102.03406</p>\n<p><br></p>\n<p>Imitating Interactive Intelligence</p>\n<p>https://arxiv.org/abs/2012.05672</p>\n<p>https://www.deepmind.com/publications/imitating-interactive-intelligence</p>\n<p><br></p>\n<p>Impact of Pretraining Term Frequencies on Few-Shot Reasoning [Yasaman Razeghi]</p>\n<p>https://arxiv.org/abs/2202.07206</p>\n<p><br></p>\n<p>Big bench dataset</p>\n<p>https://github.com/google/BIG-bench</p>\n<p><br></p>\n<p>Teaching Autoregressive Language Models Complex Tasks By Demonstration [Recchia]</p>\n<p>https://arxiv.org/pdf/2109.02102.pdf</p>\n<p><br></p>\n<p>Wason selection task</p>\n<p>https://en.wikipedia.org/wiki/Wason_selection_task</p>\n<p><br></p>\n<p>Gary Lupyan</p>\n<p>https://psych.wisc.edu/staff/lupyan-gary/</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/50592539/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-3-14%2F9316a669-8858-5ef2-5fe8-a0b9dc7d09b7.mp3"
}