{
    "Id": "7F-3E-06-68-82-90-12-22-42-05-29-24-6A-26-C7-98-1A-C3-98-2B-BB-F3-04-A3-AE-48-22-81-B6-E7-71-FB",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "Neel Nanda - Mechanistic Interpretability ",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Neel-Nanda---Mechanistic-Interpretability-e25sibc",
    "Description": "<p>In this wide-ranging conversation, Tim Scarfe interviews Neel Nanda, a researcher at DeepMind working on mechanistic interpretability, which aims to understand the algorithms and representations learned by machine learning models. Neel discusses how models can represent their thoughts using motifs, circuits, and linear directional features which are often communicated via a &quot;residual stream&quot;, an information highway models use to pass information between layers.</p>\n<p>Neel argues that &quot;superposition&quot;, the ability for models to represent more features than they have neurons, is one of the biggest open problems in interpretability. This is because superposition thwarts our ability to understand models by decomposing them into individual units of analysis. Despite this, Neel remains optimistic that ambitious interpretability is possible, citing examples like his work reverse engineering how models do modular addition. However, Neel notes we must start small, build rigorous foundations, and not assume our theoretical frameworks perfectly match reality.</p>\n<p>The conversation turns to whether models can have goals or agency, with Neel arguing they likely can based on heuristics like models executing long term plans towards some objective. However, we currently lack techniques to build models with specific goals, meaning any goals would likely be learned or emergent. Neel highlights how induction heads, circuits models use to track long range dependencies, seem crucial for phenomena like in-context learning to emerge.</p>\n<p>On the existential risks from AI, Neel believes we should avoid overly confident claims that models will or will not be dangerous, as we do not understand them enough to make confident theoretical assertions. However, models could pose risks through being misused, having undesirable emergent properties, or being imperfectly aligned. Neel argues we must pursue rigorous empirical work to better understand and ensure model safety, avoid &quot;philosophizing&quot; about definitions of intelligence, and focus on ensuring researchers have standards for what it means to decide a system is &quot;safe&quot; before deploying it. Overall, a thoughtful conversation on one of the most important issues of our time.</p>\n<p><br></p>\n<p>Support us! https://www.patreon.com/mlst </p>\n<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>\n<p>Twitter: https://twitter.com/MLStreetTalk</p>\n<p><br></p>\n<p>Neel Nanda: https://www.neelnanda.io/</p>\n<p><br></p>\n<p>TOC</p>\n<p>[00:00:00] Introduction and Neel Nanda&#39;s Interests (walk and talk)</p>\n<p>[00:03:15] Mechanistic Interpretability: Reverse Engineering Neural Networks</p>\n<p>[00:13:23] Discord questions</p>\n<p>[00:21:16] Main interview kick-off in studio</p>\n<p>[00:49:26] Grokking and Sudden Generalization</p>\n<p>[00:53:18] The Debate on Systematicity and Compositionality</p>\n<p>[01:19:16] How do ML models represent their thoughts</p>\n<p>[01:25:51] Do Large Language Models Learn World Models?</p>\n<p>[01:53:36] Superposition and Interference in Language Models</p>\n<p>[02:43:15] Transformers discussion</p>\n<p>[02:49:49] Emergence and In-Context Learning</p>\n<p>[03:20:02] Superintelligence/XRisk discussion</p>\n<p><br></p>\n<p>Transcript: https://docs.google.com/document/d/1FK1OepdJMrqpFK-_1Q3LQN6QLyLBvBwWW_5z8WrS1RI/edit?usp=sharing</p>\n<p>Refs: https://docs.google.com/document/d/115dAroX0PzSduKr5F1V4CWggYcqIoSXYBhcxYktCnqY/edit?usp=sharing</p>\n<p><br></p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/72288044/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-5-18%2Fd25a054b-86a0-d2b6-dc25-292faf13246d.mp3"
}