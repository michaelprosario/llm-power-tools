{
    "Id": "F5-7F-A6-45-90-03-8E-94-C0-8A-81-4D-35-74-04-E7-60-05-80-C6-99-9C-18-36-5A-E8-23-DF-85-4D-06-CC",
    "ContentSourceId": "7c098d5f-afc6-488d-b7ac-b49867303173",
    "Title": "Deploying Scalable Machine Learning Models for Long-Term Sustainability",
    "SourceUrl": "https://thenewstack.simplecast.com/episodes/deploying-scalable-machine-learning-models-for-long-term-sustainability-bOKL10GT",
    "Description": "<p>As machine learning models proliferate and become sophisticated, deploying them to the cloud becomes increasingly expensive. This challenge of optimizing the model also impacts the scale and requires the flexibility to move the models to different hardware like Graphic Processing Units (GPUs) or Central Processing Units (CPUs) to gain more advantage. The ability to accelerate the deployment of machine learning models to the cloud or edge at scale is shifting the way organizations build next-generation AI models and applications. And being able to optimize these models quickly to save costs and sustain them over time is moving to the forefront for many developers.</p><p>In this episode of The New Stack Makers podcast recorded at <a href=\"https://reinvent.awsevents.com/\">AWS re:Invent</a>, <a href=\"https://www.linkedin.com/in/luis-ceze-50b2314/\">Luis Ceze</a>, co-founder and CEO of <a href=\"https://octoml.ai/\">OctoML</a> talks about how to optimize and deploy machine learning models on any hardware, cloud or edge devices.</p><p><a href=\"/author/alex/\">Alex Williams</a>, founder and publisher of The New Stack hosted this podcast.</p>\n",
    "EnclosureUrl": "https://afp-922713-injected.calisto.simplecastaudio.com/5672b58f-7201-4e0e-b0af-da702259d97f/episodes/05049a31-4717-4112-a9b7-f0bcbd25b602/audio/128/default.mp3?aid=rss_feed&awCollectionId=5672b58f-7201-4e0e-b0af-da702259d97f&awEpisodeId=05049a31-4717-4112-a9b7-f0bcbd25b602&feed=IgzWks06"
}