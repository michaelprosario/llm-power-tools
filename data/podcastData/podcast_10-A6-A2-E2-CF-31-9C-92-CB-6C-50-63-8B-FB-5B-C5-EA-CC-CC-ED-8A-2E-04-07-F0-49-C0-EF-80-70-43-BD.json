{
    "Id": "10-A6-A2-E2-CF-31-9C-92-CB-6C-50-63-8B-FB-5B-C5-EA-CC-CC-ED-8A-2E-04-07-F0-49-C0-EF-80-70-43-BD",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "J\u00fcrgen Schmidhuber - Neural and Non-Neural AI, Reasoning, Transformers, and LSTMs",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Jrgen-Schmidhuber---Neural-and-Non-Neural-AI--Reasoning--Transformers--and-LSTMs-e2nnmjl",
    "Description": "<p>J\u00fcrgen Schmidhuber, the father of generative AI shares his groundbreaking work in deep learning and artificial intelligence. In this exclusive interview, he discusses the history of AI, some of his contributions to the field, and his vision for the future of intelligent machines. Schmidhuber offers unique insights into the exponential growth of technology and the potential impact of AI on humanity and the universe.</p>\n<p><br></p>\n<p>YT version: https://youtu.be/DP454c1K_vQ</p>\n<p><br></p>\n<p>MLST is sponsored by Brave:</p>\n<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>\n<p><br></p>\n<p>TOC</p>\n<p>00:00:00 Intro</p>\n<p>00:03:38 Reasoning</p>\n<p>00:13:09 Potential AI Breakthroughs Reducing Computation Needs</p>\n<p>00:20:39 Memorization vs. Generalization in AI</p>\n<p>00:25:19 Approach to the ARC Challenge</p>\n<p>00:29:10 Perceptions of Chat GPT and AGI</p>\n<p>00:58:45 Abstract Principles of Jurgen&#39;s Approach</p>\n<p>01:04:17 Analogical Reasoning and Compression</p>\n<p>01:05:48 Breakthroughs in 1991: the P, the G, and the T in ChatGPT and Generative AI</p>\n<p>01:15:50 Use of LSTM in Language Models by Tech Giants</p>\n<p>01:21:08 Neural Network Aspect Ratio Theory</p>\n<p>01:26:53 Reinforcement Learning Without Explicit Teachers</p>\n<p><br></p>\n<p>Refs:</p>\n<p>\u2605 &quot;Annotated History of Modern AI and Deep Learning&quot; (2022 survey by Schmidhuber):</p>\n<p>\u2605 Chain Rule For Backward Credit Assignment (Leibniz, 1676)</p>\n<p>\u2605 First Neural Net / Linear Regression / Shallow Learning (Gauss &amp; Legendre, circa 1800)</p>\n<p>\u2605 First 20th Century Pioneer of Practical AI (Quevedo, 1914)</p>\n<p>\u2605 First Recurrent NN (RNN) Architecture (Lenz, Ising, 1920-1925)</p>\n<p>\u2605 AI Theory: Fundamental Limitations of Computation and Computation-Based AI (G\u00f6del, 1931-34)</p>\n<p>\u2605 Unpublished ideas about evolving RNNs (Turing, 1948)</p>\n<p>\u2605 Multilayer Feedforward NN Without Deep Learning (Rosenblatt, 1958)</p>\n<p>\u2605 First Published Learning RNNs (Amari and others, ~1972)</p>\n<p>\u2605 First Deep Learning (Ivakhnenko &amp; Lapa, 1965)</p>\n<p>\u2605 Deep Learning by Stochastic Gradient Descent (Amari, 1967-68)</p>\n<p>\u2605 ReLUs (Fukushima, 1969)</p>\n<p>\u2605 Backpropagation (Linnainmaa, 1970); precursor (Kelley, 1960)</p>\n<p>\u2605 Backpropagation for NNs (Werbos, 1982)</p>\n<p>\u2605 First Deep Convolutional NN (Fukushima, 1979); later combined with Backprop (Waibel 1987, Zhang 1988).</p>\n<p>\u2605 Metalearning or Learning to Learn (Schmidhuber, 1987)</p>\n<p>\u2605 Generative Adversarial Networks / Artificial Curiosity / NN Online Planners (Schmidhuber, Feb 1990; see the G in Generative AI and ChatGPT)</p>\n<p>\u2605 NNs Learn to Generate Subgoals and Work on Command (Schmidhuber, April 1990)</p>\n<p>\u2605 NNs Learn to Program NNs: Unnormalized Linear Transformer (Schmidhuber, March 1991; see the T in ChatGPT)</p>\n<p>\u2605 Deep Learning by Self-Supervised Pre-Training. Distilling NNs (Schmidhuber, April 1991; see the P in ChatGPT)</p>\n<p>\u2605 Experiments with Pre-Training; Analysis of Vanishing/Exploding Gradients, Roots of Long Short-Term Memory / Highway Nets / ResNets (Hochreiter, June 1991, further developed 1999-2015 with other students of Schmidhuber)</p>\n<p>\u2605 LSTM journal paper (1997, most cited AI paper of the 20th century)</p>\n<p>\u2605 xLSTM (Hochreiter, 2024)</p>\n<p>\u2605 Reinforcement Learning Prompt Engineer for Abstract Reasoning and Planning (Schmidhuber 2015)</p>\n<p>\u2605 Mindstorms in Natural Language-Based Societies of Mind (2023 paper by Schmidhuber&#39;s team)</p>\n<p> https://arxiv.org/abs/2305.17066</p>\n<p>\u2605 Bremermann&#39;s physical limit of computation (1982)</p>\n<p><br></p>\n<p>EXTERNAL LINKS</p>\n<p>CogX 2018 - Professor Juergen Schmidhuber</p>\n<p>https://www.youtube.com/watch?v=17shdT9-wuA</p>\n<p>Discovering Neural Nets with Low Kolmogorov Complexity and High Generalization Capability (Neural Networks, 1997)</p>\n<p>https://sferics.idsia.ch/pub/juergen/loconet.pdf</p>\n<p>The paradox at the heart of mathematics: G\u00f6del&#39;s Incompleteness Theorem - Marcus du Sautoy</p>\n<p>https://www.youtube.com/watch?v=I4pQbo5MQOs</p>\n<p>(Refs truncated, full version on YT VD)</p>\n<p><br></p>\n<p><br></p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/91002933/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-7-28%2Fcf47b479-3c88-d4c3-62f1-009ef82e37c6.mp3"
}