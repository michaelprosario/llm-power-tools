{
    "Id": "55-06-3A-80-36-95-F4-85-6C-D7-ED-72-BF-A9-2F-95-A8-D2-B2-82-2B-3C-16-3B-6C-C1-27-DC-15-47-7C-35",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "#039 - Lena Voita - NLP",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/039---Lena-Voita---NLP-epcj2q",
    "Description": "<p>ena Voita is a Ph.D. student at the University of Edinburgh and University of Amsterdam. Previously, She was a research scientist at Yandex Research and worked closely with the Yandex Translate team. She still teaches NLP at the Yandex School of Data Analysis. She has created an exciting new NLP course on her website lena-voita.github.io which you folks need to check out! She has one of the most well presented blogs we have ever seen, where she discusses her research in an easily digestable manner. Lena has been investigating many fascinating topics in machine learning and NLP. Today we are going to talk about three of her papers and corresponding blog articles;</p>\n<p><br></p>\n<p>Source and Target Contributions to NMT Predictions -- Where she talks about the influential dichotomy between the source and the prefix of neural translation models.</p>\n<p>https://arxiv.org/pdf/2010.10907.pdf</p>\n<p>https://lena-voita.github.io/posts/source_target_contributions_to_nmt.html</p>\n<p><br></p>\n<p>Information-Theoretic Probing with MDL -- Where Lena proposes a technique of evaluating a model using the minimum description length or Kolmogorov complexity of labels given representations rather than something basic like accuracy</p>\n<p>https://arxiv.org/pdf/2003.12298.pdf</p>\n<p>https://lena-voita.github.io/posts/mdl_probes.html</p>\n<p><br></p>\n<p>Evolution of Representations in the Transformer - Lena investigates the evolution of representations of individual tokens in Transformers -- trained with different training objectives (MT, LM, MLM)&nbsp;</p>\n<p>https://arxiv.org/abs/1909.01380</p>\n<p>https://lena-voita.github.io/posts/emnlp19_evolution.html</p>\n<p><br></p>\n<p>Panel Dr. Tim Scarfe, Yannic Kilcher, Sayak Paul</p>\n<p><br></p>\n<p>00:00:00 Kenneth Stanley / Greatness can not be planned house keeping</p>\n<p>00:21:09 Kilcher intro</p>\n<p>00:28:54 Hello Lena</p>\n<p>00:29:21 Tim - Lenas NMT paper</p>\n<p>00:35:26 Tim - Minimum Description Length / Probe paper</p>\n<p>00:40:12 Tim - Evolution of representations</p>\n<p>00:46:40 Lenas NLP course</p>\n<p>00:49:18 The peppermint tea situation&nbsp;</p>\n<p>00:49:28 Main Show Kick Off&nbsp;</p>\n<p>00:50:22 Hallucination vs exposure bias&nbsp;</p>\n<p>00:53:04 Lenas focus on explaining the models not SOTA chasing</p>\n<p>00:56:34 Probes paper and NLP intepretability</p>\n<p>01:02:18 Why standard probing doesnt work</p>\n<p>01:12:12 Evolutions of representations paper</p>\n<p>01:23:53 BERTScore &nbsp;and BERT Rediscovers the Classical NLP Pipeline paper</p>\n<p>01:25:10 Is the shifting encoding context because of BERT bidirectionality</p>\n<p>01:26:43 Objective defines which information we lose on input</p>\n<p>01:27:59 How influential is the dataset?</p>\n<p>01:29:42 Where is the community going wrong?</p>\n<p>01:31:55 Thoughts on GOFAI/Understanding in NLP?</p>\n<p>01:36:38 Lena's NLP course&nbsp;</p>\n<p>01:47:40 How to foster better learning / understanding</p>\n<p>01:52:17 Lena's toolset and languages</p>\n<p>01:54:12 Mathematics is all you need</p>\n<p>01:56:03 Programming languages</p>\n<p><br></p>\n<p>https://lena-voita.github.io/</p>\n<p>https://www.linkedin.com/in/elena-voita/</p>\n<p>https://scholar.google.com/citations?user=EcN9o7kAAAAJ&amp;hl=ja</p>\n<p>https://twitter.com/lena_voita</p>\n<p><br></p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/25627162/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-0-23%2Fd0a06188-ce34-8b77-79bf-d0721a276fc6.mp3"
}