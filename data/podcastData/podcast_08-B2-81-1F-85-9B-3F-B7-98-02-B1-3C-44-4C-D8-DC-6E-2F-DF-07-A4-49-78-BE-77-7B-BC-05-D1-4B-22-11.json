{
    "Id": "08-B2-81-1F-85-9B-3F-B7-98-02-B1-3C-44-4C-D8-DC-6E-2F-DF-07-A4-49-78-BE-77-7B-BC-05-D1-4B-22-11",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer-ee9nch",
    "Description": "<p>In this episode of Machine Learning Street Talk, Tim Scarfe, Yannic Kilcher and Connor Shorten chat about Large-scale Transfer Learning in Natural Language Processing. The Text-to-Text Transfer Transformer (T5) model from Google AI does an exhaustive survey of what\u2019s important for Transfer Learning in NLP and what\u2019s not. In this conversation, we go through the key takeaways of the paper, text-to-text input/output format, architecture choice, dataset size and composition, fine-tuning strategy, and how to best use more computation.</p>\n<p>Beginning with these topics, we diverge into exciting ideas such as embodied cognition, meta-learning, and the measure of intelligence. We are still beginning our podcast journey and really appreciate any feedback from our listeners. Is the chat too technical? Do you prefer group discussions, interviewing experts, or chats between the three of us? Thanks for watching and if you haven\u2019t already, Please Subscribe!</p>\n<p>Paper Links discussed in the chat:</p>\n<p>Text-to-Text Transfer Transformer: https://arxiv.org/abs/1910.10683</p>\n<p>Experience Grounds Language (relevant to divergent discussion about embodied cognition): https://arxiv.org/pdf/2004.10151.pdf</p>\n<p>On the Measure of Intelligence: https://arxiv.org/abs/1911.01547</p>\n<p>Train Large, Then Compress: https://arxiv.org/pdf/2002.11794.pdf</p>\n<p>Scaling Laws for Neural Language Models: https://arxiv.org/pdf/2001.08361.pdf</p>\n<p>The Illustrated Transformer: http://jalammar.github.io/illustrated...</p>\n<p>ELECTRA: https://arxiv.org/pdf/2003.10555.pdf</p>\n<p>Transformer-XL: https://arxiv.org/pdf/1901.02860.pdf</p>\n<p>Reformer: The Efficient Transformer: https://openreview.net/pdf?id=rkgNKkHtvB</p>\n<p>The Evolved Transformer: https://arxiv.org/pdf/1901.11117.pdf</p>\n<p>DistilBERT: https://arxiv.org/pdf/1910.01108.pdf</p>\n<p>How to generate text (HIGHLY RECOMMEND): https://huggingface.co/blog/how-to-ge...</p>\n<p>Tokenizers: https://blog.floydhub.com/tokenization-nlp/</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/13998929/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-4-19%2F74746811-44100-2-b702b6c9a9d44.mp3"
}