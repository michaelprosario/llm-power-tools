{
    "Id": "9F-AD-EC-24-A7-E0-87-55-C4-F7-A9-27-67-8C-3A-92-02-3A-D4-BE-A1-15-21-3D-EB-17-7F-A2-06-28-94-47",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "NLP is not NLU and GPT-3 - Walid Saba",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/NLP-is-not-NLU-and-GPT-3---Walid-Saba-em16v1",
    "Description": "<p>#machinelearning</p>\n<p>This week Dr. Tim Scarfe, Dr. Keith Duggar and Yannic Kilcher speak with veteran NLU expert Dr. Walid Saba.&nbsp;</p>\n<p>Walid is an old-school AI expert. He is a polymath, a neuroscientist, psychologist, linguist, &nbsp;philosopher, statistician, and logician. He thinks the missing information problem and lack of a typed ontology is the key issue with NLU, not sample efficiency or generalisation. He is a big critic of the deep learning movement and BERTology. We also cover GPT-3 in some detail in today's session, covering Luciano Floridi's recent article \"GPT\u20113: Its Nature, Scope, Limits, and Consequences\" and a commentary on the incredible power of GPT-3 to perform tasks with just a few examples including the Yann LeCun commentary on Facebook and Hackernews.&nbsp;</p>\n<p>Time stamps on the YouTube version</p>\n<p>0:00:00 Walid intro&nbsp;</p>\n<p>00:05:03 Knowledge acquisition bottleneck&nbsp;</p>\n<p>00:06:11 Language is ambiguous&nbsp;</p>\n<p>00:07:41 Language is not learned&nbsp;</p>\n<p>00:08:32 Language is a formal language&nbsp;</p>\n<p>00:08:55 Learning from data doesn\u2019t work &nbsp;</p>\n<p>00:14:01 Intelligence&nbsp;</p>\n<p>00:15:07 Lack of domain knowledge these days&nbsp;</p>\n<p>00:16:37 Yannic Kilcher thuglife comment&nbsp;</p>\n<p>00:17:57 Deep learning assault&nbsp;</p>\n<p>00:20:07 The way we evaluate language models is flawed&nbsp;</p>\n<p>00:20:47 Humans do type checking&nbsp;</p>\n<p>00:23:02 Ontologic&nbsp;</p>\n<p>00:25:48 Comments On GPT3&nbsp;</p>\n<p>00:30:54 Yann lecun and reddit&nbsp;</p>\n<p>00:33:57 Minds and machines - Luciano&nbsp;</p>\n<p>00:35:55 Main show introduction&nbsp;</p>\n<p>00:39:02 Walid introduces himself&nbsp;</p>\n<p>00:40:20 science advances one funeral at a time&nbsp;</p>\n<p>00:44:58 Deep learning obsession syndrome and inception&nbsp;</p>\n<p>00:46:14 BERTology / empirical methods are not NLU&nbsp;</p>\n<p>00:49:55 Pattern recognition vs domain reasoning, is the knowledge in the data&nbsp;</p>\n<p>00:56:04 Natural language understanding is about decoding and not compression, it's not learnable.&nbsp;</p>\n<p>01:01:46 Intelligence is about not needing infinite amounts of time&nbsp;</p>\n<p>01:04:23 We need an explicit ontological structure to understand anything&nbsp;</p>\n<p>01:06:40 Ontological concepts&nbsp;</p>\n<p>01:09:38 Word embeddings&nbsp;</p>\n<p>01:12:20 There is power in structure&nbsp;</p>\n<p>01:15:16 Language models are not trained on pronoun disambiguation and resolving scopes&nbsp;</p>\n<p>01:17:33 The information is not in the data&nbsp;</p>\n<p>01:19:03 Can we generate these rules on the fly? Rules or data?&nbsp;</p>\n<p>01:20:39 The missing data problem is key&nbsp;</p>\n<p>01:21:19 Problem with empirical methods and lecunn reference&nbsp;</p>\n<p>01:22:45 Comparison with meatspace (brains)&nbsp;</p>\n<p>01:28:16 The knowledge graph game, is knowledge constructed or discovered&nbsp;</p>\n<p>01:29:41 How small can this ontology of the world be?&nbsp;</p>\n<p>01:33:08 Walids taxonomy of understanding&nbsp;</p>\n<p>01:38:49 The trend seems to be, less rules is better not the othe way around?&nbsp;</p>\n<p>01:40:30 Testing the latest NLP models with entailment&nbsp;</p>\n<p>01:42:25 Problems with the way we evaluate NLP&nbsp;</p>\n<p>01:44:10 Winograd Schema challenge&nbsp;</p>\n<p>01:45:56 All you need to know now is how to build neural networks, lack of rigour in ML research&nbsp;</p>\n<p>01:50:47 Is everything learnable&nbsp;</p>\n<p>01:53:02 &nbsp;How should we elevate language systems?&nbsp;</p>\n<p>01:54:04 10 big problems in language (missing information)&nbsp;</p>\n<p>01:55:59 Multiple inheritance is wrong&nbsp;</p>\n<p>01:58:19 Language is ambiguous&nbsp;</p>\n<p>02:01:14 How big would our world ontology need to be?&nbsp;</p>\n<p>02:05:49 How to learn more about NLU&nbsp;</p>\n<p>02:09:10 AlphaGo&nbsp;</p>\n<p><br></p>\n<p>Walid's blog: https://medium.com/@ontologik</p>\n<p>LinkedIn: https://www.linkedin.com/in/walidsaba/</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/22108577/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-10-4%2F63c66f58-056e-d167-7cb1-eca08f19b9d9.mp3"
}