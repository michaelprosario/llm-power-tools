{
    "Id": "59-05-A8-D0-CA-61-F5-30-6B-1D-BA-26-41-46-13-16-E1-8F-F8-CF-8B-76-C2-54-15-DB-65-E1-F7-31-21-C2",
    "ContentSourceId": "03e85e81-f3bb-4c2c-944d-b6722870407b",
    "Title": "747: Technical Intro to Transformers and LLMs, with Kirill Eremenko",
    "SourceUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD3379839131.mp3?updated=1716818982",
    "Description": "\n        <p>Attention and transformers in LLMs, the five stages of data processing, and a brand-new Large Language Models A-Z course: Kirill Eremenko joins host Jon Krohn to explore what goes into well-crafted LLMs, what makes Transformers so powerful, and how to succeed as a data scientist in this new age of generative AI.<br><br>This episode is brought to you by <a href=\"https://hpe.com/ezmeral/chatwithyourdata\">Intel and HPE Ezmeral Software Solutions</a>, and by <a href=\"https://prophetsofai.com\">Prophets of AI</a>, the leading agency for AI experts. Interested in sponsoring a SuperDataScience Podcast episode? Visit <a href=\"https://passionfroot.me/superdatascience\">passionfroot.me/superdatascience</a> for sponsorship information.<br><br>In this episode you will learn:<br>\u2022 Supply and demand in AI recruitment [08:30]<br>\u2022 Kirill and Hadelin's new course on LLMs, \u201cLarge Language Models (LLMs), Transformers &amp; GPT A-Z\u201d [15:37]<br>\u2022 The learning difficulty in understanding LLMs [19:46]<br>\u2022 The basics of LLMs [22:00]<br>\u2022 The five building blocks of transformer architecture [36:29]<br>- 1: Input embedding [44:10]<br>- 2: Positional encoding [50:46]<br>- 3: Attention mechanism [54:04]<br>- 4: Feedforward neural network [1:16:17]<br>- 5: Linear transformation and softmax [1:19:16]<br>\u2022 Inference vs training time [1:29:12]<br>\u2022 Why transformers are so powerful [1:49:22]<br><br>Additional materials: <a href=\"https://www.superdatascience.com/747\">www.superdatascience.com/747</a></p>\n      ",
    "EnclosureUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD3379839131.mp3?updated=1716818982"
}