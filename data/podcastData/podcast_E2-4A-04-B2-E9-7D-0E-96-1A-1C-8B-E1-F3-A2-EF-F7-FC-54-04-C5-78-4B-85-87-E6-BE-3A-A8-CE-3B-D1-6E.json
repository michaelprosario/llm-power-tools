{
    "Id": "E2-4A-04-B2-E9-7D-0E-96-1A-1C-8B-E1-F3-A2-EF-F7-FC-54-04-C5-78-4B-85-87-E6-BE-3A-A8-CE-3B-D1-6E",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "The Compendium - Connor Leahy and Gabriel Alfour",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/The-Compendium---Connor-Leahy-and-Gabriel-Alfour-e30s7s0",
    "Description": "<p>Connor Leahy and Gabriel Alfour, AI researchers from Conjecture and authors of &quot;The Compendium,&quot; joinus for a critical discussion centered on Artificial Superintelligence (ASI) safety and governance. Drawing from their comprehensive analysis in &quot;The Compendium,&quot; they articulate a stark warning about the existential risks inherent in uncontrolled AI development, framing it through the lens of &quot;intelligence domination&quot;\u2014where a sufficiently advanced AI could subordinate humanity, much like humans dominate less intelligent species.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>TRANSCRIPT + REFS + NOTES:</p><p>https://www.dropbox.com/scl/fi/p86l75y4o2ii40df5t7no/Compendium.pdf?rlkey=tukczgf3flw133sr9rgss0pnj&amp;dl=0</p><p><br></p><p>https://www.thecompendium.ai/</p><p>https://en.wikipedia.org/wiki/Connor_Leahy</p><p>https://www.conjecture.dev/about</p><p>https://substack.com/@gabecc\u200b</p><p><br></p><p>TOC:</p><p>1. AI Intelligence and Safety Fundamentals</p><p> [00:00:00] 1.1 Understanding Intelligence and AI Capabilities</p><p>   [00:06:20] 1.2 Emergence of Intelligence and Regulatory Challenges</p><p>   [00:10:18] 1.3 Human vs Animal Intelligence Debate</p><p>   [00:18:00] 1.4 AI Regulation and Risk Assessment Approaches</p><p>   [00:26:14] 1.5 Competing AI Development Ideologies</p><p><br></p><p>2. Economic and Social Impact</p><p>   [00:29:10] 2.1 Labor Market Disruption and Post-Scarcity Scenarios</p><p>   [00:32:40] 2.2 Institutional Frameworks and Tech Power Dynamics</p><p>   [00:37:40] 2.3 Ethical Frameworks and AI Governance Debates</p><p>   [00:40:52] 2.4 AI Alignment Evolution and Technical Challenges</p><p><br></p><p>3. Technical Governance Framework</p><p>   [00:55:07] 3.1 Three Levels of AI Safety: Alignment, Corrigibility, and Boundedness</p><p>   [00:55:30] 3.2 Challenges of AI System Corrigibility and Constitutional Models</p><p>   [00:57:35] 3.3 Limitations of Current Boundedness Approaches</p><p>   [00:59:11] 3.4 Abstract Governance Concepts and Policy Solutions</p><p><br></p><p>4. Democratic Implementation and Coordination</p><p>   [00:59:20] 4.1 Governance Design and Measurement Challenges</p><p>   [01:00:10] 4.2 Democratic Institutions and Experimental Governance</p><p>   [01:14:10] 4.3 Political Engagement and AI Safety Advocacy</p><p>   [01:25:30] 4.4 Practical AI Safety Measures and International Coordination</p><p><br></p><p>CORE REFS:</p><p>[00:01:45] The Compendium (2023), Leahy et al.</p><p>https://pdf.thecompendium.ai/the_compendium.pdf</p><p><br></p><p>[00:06:50] Geoffrey Hinton Leaves Google, BBC News</p><p>https://www.bbc.com/news/world-us-canada-65452940</p><p><br></p><p>[00:10:00] ARC-AGI, Chollet</p><p>https://arcprize.org/arc-agi</p><p><br></p><p>[00:13:25] A Brief History of Intelligence, Bennett</p><p>https://www.amazon.com/Brief-History-Intelligence-Humans-Breakthroughs/dp/0063286343</p><p><br></p><p>[00:25:35] Statement on AI Risk, Center for AI Safety</p><p>https://www.safe.ai/work/statement-on-ai-risk</p><p><br></p><p>[00:26:15] Machines of Love and Grace, Amodei</p><p>https://darioamodei.com/machines-of-loving-grace</p><p><br></p><p>[00:26:35] The Techno-Optimist Manifesto, Andreessen</p><p>https://a16z.com/the-techno-optimist-manifesto/</p><p><br></p><p>[00:31:55] Techno-Feudalism, Varoufakis</p><p>https://www.amazon.co.uk/Technofeudalism-Killed-Capitalism-Yanis-Varoufakis/dp/1847927270</p><p><br></p><p>[00:42:40] Introducing Superalignment, OpenAI</p><p>https://openai.com/index/introducing-superalignment/</p><p><br></p><p>[00:47:20] Three Laws of Robotics, Asimov</p><p>https://www.britannica.com/topic/Three-Laws-of-Robotics</p><p><br></p><p>[00:50:00] Symbolic AI (GOFAI), Haugeland</p><p>https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence</p><p><br></p><p>[00:52:30] Intent Alignment, Christiano</p><p>https://www.alignmentforum.org/posts/HEZgGBZTpT4Bov7nH/mapping-the-conceptual-territory-in-ai-existential-safety</p><p><br></p><p>[00:55:10] Large Language Model Alignment: A Survey, Jiang et al.</p><p>http://arxiv.org/pdf/2309.15025</p><p><br></p><p>[00:55:40] Constitutional Checks and Balances, Bok</p><p>https://plato.stanford.edu/entries/montesquieu/</p><p>&lt;trunc, see PDF&gt;</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/100588864/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-30%2F2f821d64-3ab9-3534-5bc2-ec24a7e20e6a.mp3"
}