{
    "Id": "14-AC-6B-E8-66-33-5B-32-5D-96-F6-EA-41-BB-AB-A9-C6-A8-20-7D-E2-22-81-EA-D8-B9-86-E9-F5-8D-04-E4",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "The Lottery Ticket Hypothesis with Jonathan Frankle",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/The-Lottery-Ticket-Hypothesis-with-Jonathan-Frankle-ee9sg4",
    "Description": "<p>In this episode of Machine Learning Street Talk, we chat with Jonathan Frankle, author of The Lottery Ticket Hypothesis. Frankle has continued researching Sparse Neural Networks, Pruning, and Lottery Tickets leading to some really exciting follow-on papers! This chat discusses some of these papers such as Linear Mode Connectivity, Comparing and Rewinding and Fine-tuning in Neural Network Pruning, and more (full list of papers linked below). We also chat about how Jonathan got into Deep Learning research, his Information Diet, and work on developing Technology Policy for Artificial Intelligence!&nbsp;</p>\n<p>This was a really fun chat, I hope you enjoy listening to it and learn something from it!</p>\n<p>Thanks for watching and please subscribe!</p>\n<p>Huge thanks to everyone on r/MachineLearning who asked questions!</p>\n<p>Paper Links discussed in the chat:</p>\n<p>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks: https://arxiv.org/abs/1803.03635</p>\n<p>Linear Mode Connectivity and the Lottery Ticket Hypothesis: https://arxiv.org/abs/1912.05671</p>\n<p>Dissecting Pruned Neural Networks: https://arxiv.org/abs/1907.00262</p>\n<p>Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs: https://arxiv.org/abs/2003.00152</p>\n<p>What is the State of Neural Network Pruning? https://arxiv.org/abs/2003.03033</p>\n<p>The Early Phase of Neural Network Training: https://arxiv.org/abs/2002.10365</p>\n<p>Comparing Rewinding and Fine-tuning in Neural Network Pruning: https://arxiv.org/abs/2003.02389</p>\n<p>(Also Mentioned)</p>\n<p>Block-Sparse GPU Kernels: https://openai.com/blog/block-sparse-gpu-kernels/</p>\n<p>Balanced Sparsity for Efficient DNN Inference on GPU: https://arxiv.org/pdf/1811.00206.pdf</p>\n<p>Playing the Lottery with Rewards and Multiple Languages: Lottery Tickets in RL and NLP: https://arxiv.org/pdf/1906.02768.pdf</p>\n<p>r/MachineLearning question list: https://www.reddit.com/r/MachineLearning/comments/g9jqe0/d_lottery_ticket_hypothesis_ask_the_author_a/ (edited)&nbsp;</p>\n<p>#machinelearning #deeplearning</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/14004164/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-4-19%2F74809790-48000-2-6758698a387e9.mp3"
}