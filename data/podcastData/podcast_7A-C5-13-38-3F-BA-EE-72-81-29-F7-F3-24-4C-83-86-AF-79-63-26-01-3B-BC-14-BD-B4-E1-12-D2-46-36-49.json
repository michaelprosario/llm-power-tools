{
    "Id": "7A-C5-13-38-3F-BA-EE-72-81-29-F7-F3-24-4C-83-86-AF-79-63-26-01-3B-BC-14-BD-B4-E1-12-D2-46-36-49",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "#94 - ALAN CHAN - AI Alignment and Governance #NEURIPS",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/94---ALAN-CHAN---AI-Alignment-and-Governance-NEURIPS-e1sntud",
    "Description": "<p>Support us! https://www.patreon.com/mlst</p>\n<p>Alan Chan is a PhD student at Mila, the Montreal Institute for Learning Algorithms, supervised by Nicolas Le Roux. Before joining Mila, Alan was a Masters student at the Alberta Machine Intelligence Institute and the University of Alberta, where he worked with Martha White. Alan's expertise and research interests encompass value alignment and AI governance. He is currently exploring the measurement of harms from language models and the incentives that agents have to impact the world. Alan's research focuses on understanding and controlling the values expressed by machine learning models. His projects have examined the regulation of explainability in algorithmic systems, scoring rules for performative binary prediction, the effects of global exclusion in AI development, and the role of a graduate student in approaching ethical impacts in AI research. In addition, Alan has conducted research into inverse policy evaluation for value-based sequential decision-making, and the concept of \"normal accidents\" and AI systems. Alan's research is motivated by the need to align AI systems with human values, and his passion for scientific and governance work in this field. Alan's energy and enthusiasm for his field is infectious.&nbsp;</p>\n<p>This was a discussion at NeurIPS. It was in quite a loud environment so the audio quality could have been better.&nbsp;</p>\n<p>References:</p>\n<p><br></p>\n<p>The Rationalist's Guide to the Galaxy: Superintelligent AI and the Geeks Who Are Trying to Save Humanity's Future [Tim Chivers]</p>\n<p>https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795</p>\n<p><br></p>\n<p>The implausibility of intelligence explosion [Chollet]</p>\n<p>https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec</p>\n<p><br></p>\n<p>Superintelligence: Paths, Dangers, Strategies [Bostrom]</p>\n<p>https://www.amazon.co.uk/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111</p>\n<p><br></p>\n<p>A Theory of Universal Artificial Intelligence based on Algorithmic Complexity [Hutter]</p>\n<p>https://arxiv.org/abs/cs/0004001</p>\n<p><br></p>\n<p>YT version: https://youtu.be/XBMnOsv9_pk&nbsp;</p>\n<p>MLST Discord: https://discord.gg/aNPkGUQtc5&nbsp;</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/62698893/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-26%2F4605a493-e90e-e35d-f934-f0809c40a24d.mp3"
}