{
    "Id": "D0-38-51-93-F4-EB-D8-B2-68-29-B8-AA-D1-79-04-79-03-C0-9C-64-A9-F2-B0-9A-6C-A2-1A-D9-4C-72-C9-A8",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "#99 - CARLA CREMER & IGOR KRAWCZUK - X-Risk, Governance, Effective Altruism",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/99---CARLA-CREMER--IGOR-KRAWCZUK---X-Risk--Governance--Effective-Altruism-e1ugl0o",
    "Description": "<p>YT version (with references): https://www.youtube.com/watch?v=lxaTinmKxs0</p>\n<p>Support us! https://www.patreon.com/mlst</p>\n<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>\n<p><br></p>\n<p>Carla Cremer and Igor Krawczuk argue that AI risk should be understood as an old problem of politics, power and control with known solutions, and that threat models should be driven by empirical work. The interaction between FTX and the Effective Altruism community has sparked a lot of discussion about the dangers of optimization, and Carla's Vox article highlights the need for an institutional turn when taking on a responsibility like risk management for humanity.</p>\n<p><br></p>\n<p>Carla's \u201cDemocratizing Risk\u201d paper found that certain types of risks fall through the cracks if they are just categorized into climate change or biological risks. Deliberative democracy has been found to be a better way to make decisions, and AI tools can be used to scale this type of democracy and be used for good, but the transparency of these algorithms to the citizens using the platform must be taken into consideration.</p>\n<p><br></p>\n<p>Aggregating people\u2019s diverse ways of thinking about a problem and creating a risk-averse procedure gives a likely, highly probable outcome for having converged on the best policy. There needs to be a good reason to trust one organization with the risk management of humanity and all the different ways of thinking about risk must be taken into account. AI tools can help to scale this type of deliberative democracy, but the transparency of these algorithms must be taken into consideration.</p>\n<p><br></p>\n<p>The ambition of the EA community and Altruism Inc. is to protect and do risk management for the whole of humanity and this requires an institutional turn in order to do it effectively. The dangers of optimization are real, and it is essential to ensure that the risk management of humanity is done properly and ethically. By understanding the importance of aggregating people\u2019s diverse ways of thinking about a problem, and creating a risk-averse procedure, it is possible to create a likely, highly probable outcome for having converged on the best policy.</p>\n<p><br></p>\n<p>Carla Zoe Cremer</p>\n<p>https://carlacremer.github.io/</p>\n<p><br></p>\n<p>Igor Krawczuk</p>\n<p>https://krawczuk.eu/</p>\n<p><br></p>\n<p>Interviewer: Dr. Tim Scarfe</p>\n<p><br></p>\n<p>TOC:</p>\n<p>[00:00:00] Introduction: Vox article and effective altruism / FTX</p>\n<p>[00:11:12] Luciano Floridi on Governance and Risk</p>\n<p>[00:15:50] Connor Leahy on alignment</p>\n<p>[00:21:08] Ethan Caballero on scaling</p>\n<p>[00:23:23] Alignment, Values and politics</p>\n<p>[00:30:50] Singularitarians vs AI-thiests</p>\n<p>[00:41:56] Consequentialism</p>\n<p>[00:46:44] Does scale make a difference?</p>\n<p>[00:51:53] Carla's Democratising risk paper</p>\n<p>[01:04:03] Vox article - How effective altruists ignored risk</p>\n<p>[01:20:18] Does diversity breed complexity?</p>\n<p>[01:29:50] Collective rationality</p>\n<p>[01:35:16] Closing statements</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/64557528/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-1-5%2F4481f8e5-0aac-cc52-c7a5-c150602cf74f.mp3"
}