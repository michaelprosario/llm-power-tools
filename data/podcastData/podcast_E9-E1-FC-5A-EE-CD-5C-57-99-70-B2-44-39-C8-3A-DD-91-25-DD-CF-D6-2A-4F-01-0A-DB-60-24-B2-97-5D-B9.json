{
    "Id": "E9-E1-FC-5A-EE-CD-5C-57-99-70-B2-44-39-C8-3A-DD-91-25-DD-CF-D6-2A-4F-01-0A-DB-60-24-B2-97-5D-B9",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "#032- Simon Kornblith / GoogleAI - SimCLR and Paper Haul!",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/032--Simon-Kornblith--GoogleAI---SimCLR-and-Paper-Haul-endpa3",
    "Description": "<p>This week Dr. Tim Scarfe, Sayak Paul and Yannic Kilcher speak with Dr. Simon Kornblith from Google Brain (Ph.D from MIT). Simon is trying to understand how neural nets do what they do. Simon was the second author on the seminal Google AI SimCLR paper. We also cover \"Do Wide and Deep Networks learn the same things?\", \"Whats in a Loss function for Image Classification?\", &nbsp;and \"Big Self-supervised models are strong semi-supervised learners\". Simon used to be a neuroscientist and also gives us the story of his unique journey into ML.</p>\n<p><br></p>\n<p>00:00:00 Show Teaser / or \"short version\"</p>\n<p>00:18:34 Show intro</p>\n<p>00:22:11 Relationship between neuroscience and machine learning</p>\n<p>00:29:28 Similarity analysis and evolution of representations in Neural Networks</p>\n<p>00:39:55 Expressability of NNs</p>\n<p>00:42:33 Whats in a loss function for image classification</p>\n<p>00:46:52 Loss function implications for transfer learning</p>\n<p>00:50:44 SimCLR paper&nbsp;</p>\n<p>01:00:19 Contrast SimCLR to BYOL</p>\n<p>01:01:43 Data augmentation</p>\n<p>01:06:35 Universality of image representations</p>\n<p>01:09:25 Universality of augmentations</p>\n<p>01:23:04 GPT-3</p>\n<p>01:25:09 GANs for data augmentation??</p>\n<p>01:26:50 Julia language</p>\n<p><br></p>\n<p>@skornblith</p>\n<p>https://www.linkedin.com/in/simon-kornblith-54b2033a/</p>\n<p><br></p>\n<p>https://arxiv.org/abs/2010.15327</p>\n<p>Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth</p>\n<p><br></p>\n<p>https://arxiv.org/abs/2010.16402</p>\n<p>What's in a Loss Function for Image Classification?</p>\n<p><br></p>\n<p>https://arxiv.org/abs/2002.05709</p>\n<p>A Simple Framework for Contrastive Learning of Visual Representations</p>\n<p><br></p>\n<p>https://arxiv.org/abs/2006.10029</p>\n<p>Big Self-Supervised Models are Strong Semi-Supervised Learners</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/23569155/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-11-6%2F42980d04-6f8d-9944-e9dd-afb94c0e9042.mp3"
}