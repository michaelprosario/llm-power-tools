{
    "Id": "2B-7E-BD-06-DA-D7-1A-77-8E-43-33-8C-25-0B-82-A8-34-35-0E-A4-9D-E5-BF-AA-24-DA-E6-7D-0C-49-04-63",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "ROBERT MILES - \"There is a good chance this kills everyone\"",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/ROBERT-MILES---There-is-a-good-chance-this-kills-everyone-e24eio7",
    "Description": "<p>Please check out Numerai - our sponsor @</p>\n<p>https://numerai.com/mlst</p>\n<p><br></p>\n<p>Numerai is a groundbreaking platform which is taking the data science world by storm. Tim has been using Numerai to build state-of-the-art models which predict the stock market, all while being a part of an inspiring community of data scientists from around the globe. They host the Numerai Data Science Tournament, where data scientists like us use their financial dataset to predict future stock market performance.</p>\n<p><br></p>\n<p>Support us! https://www.patreon.com/mlst </p>\n<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>\n<p>Twitter: https://twitter.com/MLStreetTalk</p>\n<p><br></p>\n<p>Welcome to an exciting episode featuring an outstanding guest, Robert Miles! Renowned for his extraordinary contributions to understanding AI and its potential impacts on our lives, Robert is an artificial intelligence advocate, researcher, and YouTube sensation. He combines engaging discussions with entertaining content, captivating millions of viewers from around the world.</p>\n<p>With a strong computer science background, Robert has been actively involved in AI safety projects, focusing on raising awareness about potential risks and benefits of advanced AI systems. His YouTube channel is celebrated for making AI safety discussions accessible to a diverse audience through breaking down complex topics into easy-to-understand nuggets of knowledge, and you might also recognise him from his appearances on Computerphile.</p>\n<p>In this episode, join us as we dive deep into Robert&#39;s journey in the world of AI, exploring his insights on AI alignment, superintelligence, and the role of AI shaping our society and future. We&#39;ll discuss topics such as the limits of AI capabilities and physics, AI progress and timelines, human-machine hybrid intelligence, AI in conflict and cooperation with humans, and the convergence of AI communities.</p>\n<p><br></p>\n<p>Robert Miles: </p>\n<p> @RobertMilesAI </p>\n<p>https://twitter.com/robertskmiles</p>\n<p>https://aisafety.info/</p>\n<p><br></p>\n<p>YT version: https://www.youtube.com/watch?v=kMLKbhY0ji0</p>\n<p><br></p>\n<p>Panel:</p>\n<p>Dr. Tim Scarfe</p>\n<p>Dr. Keith Duggar</p>\n<p>Joint CTOs - https://xrai.glass/</p>\n<p><br></p>\n<p>Refs:</p>\n<p>Are Emergent Abilities of Large Language Models a Mirage? (Rylan Schaeffer)</p>\n<p>https://arxiv.org/abs/2304.15004</p>\n<p><br></p>\n<p>TOC:</p>\n<p>Intro [00:00:00]</p>\n<p>Numerai Sponsor Messsage [00:02:17]</p>\n<p>AI Alignment [00:04:27]</p>\n<p>Limits of AI Capabilities and Physics [00:18:00]</p>\n<p>AI Progress and Timelines [00:23:52]</p>\n<p>AI Arms Race and Innovation [00:31:11]</p>\n<p>Human-Machine Hybrid Intelligence [00:38:30]</p>\n<p>Understanding and Defining Intelligence [00:42:48]</p>\n<p>AI in Conflict and Cooperation with Humans [00:50:13]</p>\n<p>Interpretability and Mind Reading in AI [01:03:46]</p>\n<p>Mechanistic Interpretability and Deconfusion Research [01:05:53]</p>\n<p>Understanding the core concepts of AI [01:07:40]</p>\n<p>Moon landing analogy and AI alignment [01:09:42]</p>\n<p>Cognitive horizon and limits of human intelligence [01:11:42]</p>\n<p>Funding and focus on AI alignment [01:16:18]</p>\n<p>Regulating AI technology and potential risks [01:19:17]</p>\n<p>Aligning AI with human values and its dynamic nature [01:27:04]</p>\n<p>Cooperation and Allyship [01:29:33]</p>\n<p>Orthogonality Thesis and Goal Preservation [01:33:15]</p>\n<p>Anthropomorphic Language and Intelligent Agents [01:35:31]</p>\n<p>Maintaining Variety and Open-ended Existence [01:36:27]</p>\n<p>Emergent Abilities of Large Language Models [01:39:22]</p>\n<p>Convergence vs Emergence [01:44:04]</p>\n<p>Criticism of X-risk and Alignment Communities [01:49:40]</p>\n<p>Fusion of AI communities and addressing biases [01:52:51]</p>\n<p>AI systems integration into society and understanding them [01:53:29]</p>\n<p>Changing opinions on AI topics and learning from past videos [01:54:23]</p>\n<p>Utility functions and von Neumann-Morgenstern theorems [01:54:47]</p>\n<p>AI Safety FAQ project [01:58:06]</p>\n<p>Building a conversation agent using AI safety dataset [02:00:36]</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/70781127/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-4-21%2Fb355263e-1788-fabd-ebe6-f7ed528dd89c.mp3"
}