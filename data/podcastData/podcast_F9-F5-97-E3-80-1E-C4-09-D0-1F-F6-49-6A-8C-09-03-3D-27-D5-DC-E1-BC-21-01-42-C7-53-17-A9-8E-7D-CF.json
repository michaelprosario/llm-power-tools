{
    "Id": "F9-F5-97-E3-80-1E-C4-09-D0-1F-F6-49-6A-8C-09-03-3D-27-D5-DC-E1-BC-21-01-42-C7-53-17-A9-8E-7D-CF",
    "ContentSourceId": "03e85e81-f3bb-4c2c-944d-b6722870407b",
    "Title": "650: SparseGPT: Remove 100 Billion Parameters but Retain 100% Accuracy",
    "SourceUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD8263011108.mp3?updated=1716819159",
    "Description": "\n        <p>SparseGPT is a noteworthy one-shot pruning technique that can halve the size of large language models like GPT-3 without adversely affecting accuracy. In this episode, Jon Krohn provides an overview of this development and explains its commercial and environmental implications.</p><p>Additional materials: <a href=\"https://www.superdatascience.com/650\">www.superdatascience.com/650</a></p><p><br>Interested in sponsoring a SuperDataScience Podcast episode? Visit <a href=\"http://jonkrohn.com/podcast\">JonKrohn.com/podcast</a> for sponsorship information.</p>\n      ",
    "EnclosureUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD8263011108.mp3?updated=1716819159"
}