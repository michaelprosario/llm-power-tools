{
    "Id": "98-FF-F2-21-2E-45-C3-F1-60-6E-8E-78-2F-3F-15-C3-5E-37-E6-22-9E-28-46-F3-CE-A7-03-2D-08-0F-0A-92",
    "ContentSourceId": "03e85e81-f3bb-4c2c-944d-b6722870407b",
    "Title": "778: Mixtral 8x22B: SOTA Open-Source LLM Capabilities at a Fraction of the Compute",
    "SourceUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD5713431189.mp3?updated=1716818953",
    "Description": "\n        <p>Mixtral 8x22B is the focus on this week's Five-Minute Friday. Jon Krohn examines how this model from French AI startup Mistral leverages its mixture-of-experts architecture to redefine efficiency and specialization in AI-powered tasks. Tune in to learn about its performance benchmarks and the transformative potential of its open-source license.<br><br>Additional materials: <a href=\"https://www.superdatascience.com/778\">www.superdatascience.com/778</a><br><br>Interested in sponsoring a SuperDataScience Podcast episode? Visit <a href=\"https://passionfroot.me/superdatascience\">passionfroot.me/superdatascience</a> for sponsorship information.</p>\n      ",
    "EnclosureUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD5713431189.mp3?updated=1716818953"
}