{
    "Id": "C3-73-AC-53-BA-33-6B-5D-25-C7-B2-93-B8-D5-23-F8-28-99-51-56-7B-52-16-76-E0-EF-39-2A-BA-1D-BB-D2",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "#030 Multi-Armed Bandits and Pure-Exploration (Wouter M. Koolen)",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/030-Multi-Armed-Bandits-and-Pure-Exploration-Wouter-M--Koolen-emp42c",
    "Description": "<p>This week Dr. Tim Scarfe, Dr. Keith Duggar and Yannic Kilcher discuss multi-arm bandits and pure exploration with Dr. Wouter M. Koolen, Senior Researcher, Machine Learning group, Centrum Wiskunde &amp; Informatica.</p>\n<p><br></p>\n<p>Wouter specialises in machine learning theory, game theory, information theory, statistics and optimisation. Wouter is currently interested in pure exploration in multi-armed bandit models, game tree search, and accelerated learning in sequential decision problems. His research has been cited 1000 times, and he has been published in NeurIPS, the number 1 ML conference 14 times as well as lots of other exciting publications.</p>\n<p><br></p>\n<p>Today we are going to talk about two of the most studied settings in control, decision theory, and learning in unknown environment which are the multi-armed bandit (MAB) and reinforcement learning (RL) approaches</p>\n<p>- when can an agent stop learning and start exploiting using the knowledge it obtained</p>\n<p>- which strategy leads to minimal learning time</p>\n<p><br></p>\n<p>00:00:00 What are multi-arm bandits/show trailer</p>\n<p>00:12:55 Show introduction</p>\n<p>00:15:50 Bandits&nbsp;</p>\n<p>00:18:58 Taxonomy of decision framework approaches&nbsp;</p>\n<p>00:25:46 Exploration vs Exploitation&nbsp;</p>\n<p>00:31:43 the sharp divide between modes&nbsp;</p>\n<p>00:34:12 bandit measures of success&nbsp;</p>\n<p>00:36:44 connections to reinforcement learning&nbsp;</p>\n<p>00:44:00 when to apply pure exploration in games&nbsp;</p>\n<p>00:45:54 bandit lower bounds, a pure exploration renaissance&nbsp;</p>\n<p>00:50:21 pure exploration compiler dreams&nbsp;</p>\n<p>00:51:56 what would the PX-compiler DSL look like&nbsp;</p>\n<p>00:57:13 the long arms of the bandit&nbsp;</p>\n<p>01:00:21 causal models behind the curtain of arms&nbsp;</p>\n<p>01:02:43 adversarial bandits, arms trying to beat you&nbsp;</p>\n<p>01:05:12 bandits as an optimization problem&nbsp;</p>\n<p>01:11:39 asymptotic optimality vs practical performance&nbsp;</p>\n<p>01:15:38 pitfalls hiding under asymptotic cover&nbsp;</p>\n<p>01:18:50 adding features to bandits&nbsp;</p>\n<p>01:27:24 moderate confidence regimes &nbsp;</p>\n<p>01:30:33 algorithms choice is highly sensitive to bounds&nbsp;</p>\n<p>01:46:09 Post script: Keith interesting piece on n quantum&nbsp;</p>\n<p><br></p>\n<p>http://wouterkoolen.info</p>\n<p>https://www.cwi.nl/research-groups/ma...</p>\n<p><br></p>\n<p>#machinelearning</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/22892044/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-10-20%2Faf6a8ad0-7465-85fb-4fa3-0eeb527206a5.mp3"
}