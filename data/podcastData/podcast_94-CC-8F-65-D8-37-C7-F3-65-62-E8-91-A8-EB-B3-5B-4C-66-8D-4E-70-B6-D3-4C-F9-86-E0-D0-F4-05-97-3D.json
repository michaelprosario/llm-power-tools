{
    "Id": "94-CC-8F-65-D8-37-C7-F3-65-62-E8-91-A8-EB-B3-5B-4C-66-8D-4E-70-B6-D3-4C-F9-86-E0-D0-F4-05-97-3D",
    "ContentSourceId": "03e85e81-f3bb-4c2c-944d-b6722870407b",
    "Title": "689: Observing LLMs in Production to Automatically Catch Issues",
    "SourceUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD5664781691.mp3?updated=1716819071",
    "Description": "\n        <p>Arize's Amber Roberts and Xander Song join Jon Krohn this week, sharing invaluable insights into ML Observability, drift detection, retraining strategies, and the crucial task of ensuring fairness and ethical considerations in AI development.<br><br>This episode is brought to you by <a href=\"https://posit.co/\">Posit</a>, the open-source data science company, by <a href=\"https://go.aws/3zWS0au\">AWS Inferentia</a>, and by <a href=\"https://superdatascience.com/anaconda\">Anaconda</a>, the world's most popular Python distribution. Interested in sponsoring a SuperDataScience Podcast episode? Visit <a href=\"https://jonkrohn.com/podcast\">JonKrohn.com/podcast</a> for sponsorship information.<br><br>In this episode you will learn:<br>\u2022 What is ML Observability [05:07]<br>\u2022 What is Drift [08:18]<br>\u2022 The different kinds of model drift [15:31]<br>\u2022 How frequently production models should be retrained? [25:15]<br>\u2022 Arize's open-source product, Phoenix [30:49]<br>\u2022 How ML Observability relates to discovering model biases [50:30]<br>\u2022 Arize case studies [57:13]<br>\u2022 What is a developer advocate [1:04:51]<br><br>Additional materials: <a href=\"https://www.superdatascience.com/689\">www.superdatascience.com/689</a></p>\n      ",
    "EnclosureUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD5664781691.mp3?updated=1716819071"
}