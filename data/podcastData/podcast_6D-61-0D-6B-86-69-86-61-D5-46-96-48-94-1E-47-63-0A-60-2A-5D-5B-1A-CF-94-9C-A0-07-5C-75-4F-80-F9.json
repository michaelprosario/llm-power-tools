{
    "Id": "6D-61-0D-6B-86-69-86-61-D5-46-96-48-94-1E-47-63-0A-60-2A-5D-5B-1A-CF-94-9C-A0-07-5C-75-4F-80-F9",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "#83 Dr. ANDREW LAMPINEN (Deepmind) - Natural Language, Symbols and Grounding [NEURIPS2022 UNPLUGGED]",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/83-Dr--ANDREW-LAMPINEN-Deepmind---Natural-Language--Symbols-and-Grounding-NEURIPS2022-UNPLUGGED-e1rne53",
    "Description": "<p>First in our unplugged series live from #NeurIPS2022</p>\n<p>We discuss natural language understanding, symbol meaning and grounding and Chomsky with Dr. Andrew Lampinen from DeepMind.&nbsp;</p>\n<p>We recorded a LOT of material from NeurIPS, keep an eye out for the uploads.&nbsp;</p>\n<p><br></p>\n<p>YT version: https://youtu.be/46A-BcBbMnA</p>\n<p><br></p>\n<p>References</p>\n<p>[Paul Cisek] Beyond the computer metaphor: Behaviour as interaction</p>\n<p>https://philpapers.org/rec/CISBTC</p>\n<p><br></p>\n<p>Linguistic Competence (Chomsky reference)</p>\n<p>https://en.wikipedia.org/wiki/Linguistic_competence</p>\n<p><br></p>\n<p>[Andrew Lampinen] Can language models handle recursively nested grammatical structures? A case study on comparing models and humans</p>\n<p>https://arxiv.org/abs/2210.15303</p>\n<p><br></p>\n<p>[Fodor et al] Connectionism and Cognitive Architecture: A Critical Analysis</p>\n<p>https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf</p>\n<p><br></p>\n<p>[Melanie Mitchell et al] The Debate Over Understanding in AI's Large Language Models</p>\n<p>https://arxiv.org/abs/2210.13966</p>\n<p><br></p>\n<p>[Gary Marcus] GPT-3, Bloviator: OpenAI\u2019s language generator has no idea what it\u2019s talking about</p>\n<p>https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/</p>\n<p><br></p>\n<p>[Bender et al] On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</p>\n<p>https://dl.acm.org/doi/10.1145/3442188.3445922</p>\n<p><br></p>\n<p>[Adam Santoro, Andrew Lampinen et al] Symbolic Behaviour in Artificial Intelligence</p>\n<p>https://arxiv.org/abs/2102.03406</p>\n<p><br></p>\n<p>[Ishita Dasgupta, Lampinen et al] Language models show human-like content effects on reasoning</p>\n<p>https://arxiv.org/abs/2207.07051</p>\n<p><br></p>\n<p>REACT - Synergizing Reasoning and Acting in Language Models</p>\n<p>https://arxiv.org/pdf/2210.03629.pdf</p>\n<p>https://ai.googleblog.com/2022/11/react-synergizing-reasoning-and-acting.html</p>\n<p><br></p>\n<p>[Fabian Paischer] HELM - History Compression via Language Models in Reinforcement Learning</p>\n<p>https://ml-jku.github.io/blog/2022/helm/</p>\n<p>https://arxiv.org/abs/2205.12258</p>\n<p><br></p>\n<p>[Laura Ruis] Large language models are not zero-shot communicators</p>\n<p>https://arxiv.org/pdf/2210.14986.pdf</p>\n<p><br></p>\n<p>[Kumar] Using natural language and program abstractions to instill human inductive biases in machines</p>\n<p>https://arxiv.org/pdf/2205.11558.pdf</p>\n<p><br></p>\n<p>Juho Kim</p>\n<p>https://juhokim.com/</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/61634147/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-4%2Fc72faf62-61e3-adf3-f536-eb12931f875b.mp3"
}