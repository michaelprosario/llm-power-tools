{
    "Id": "67-4B-95-06-95-45-CC-8D-F3-C7-09-44-2D-74-1D-66-6E-ED-ED-3B-10-48-A3-D3-E7-99-FC-EB-50-69-F8-AC",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "Nicholas Carlini (Google DeepMind)",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Nicholas-Carlini-Google-DeepMind-e2tvqch",
    "Description": "<p>Nicholas Carlini from Google DeepMind offers his view of AI security, emergent LLM capabilities, and his groundbreaking model-stealing research. He reveals how LLMs can unexpectedly excel at tasks like chess and discusses the security pitfalls of LLM-generated code. </p>\n<p><br></p>\n<p>SPONSOR MESSAGES:</p>\n<p>***</p>\n<p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. </p>\n<p>https://centml.ai/pricing/</p>\n<p><br></p>\n<p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? </p>\n<p><br></p>\n<p>Goto https://tufalabs.ai/</p>\n<p>***</p>\n<p><br></p>\n<p>Transcript: https://www.dropbox.com/scl/fi/lat7sfyd4k3g5k9crjpbf/CARLINI.pdf?rlkey=b7kcqbvau17uw6rksbr8ccd8v&amp;dl=0</p>\n<p><br></p>\n<p>TOC:</p>\n<p>1. ML Security Fundamentals </p>\n<p>[00:00:00] 1.1 ML Model Reasoning and Security Fundamentals </p>\n<p>[00:03:04] 1.2 ML Security Vulnerabilities and System Design </p>\n<p>[00:08:22] 1.3 LLM Chess Capabilities and Emergent Behavior  </p>\n<p>[00:13:20] 1.4 Model Training, RLHF, and Calibration Effects  </p>\n<p><br></p>\n<p>2. Model Evaluation and Research Methods  </p>\n<p>[00:19:40] 2.1 Model Reasoning and Evaluation Metrics  </p>\n<p>[00:24:37] 2.2 Security Research Philosophy and Methodology  </p>\n<p>[00:27:50] 2.3 Security Disclosure Norms and Community Differences  </p>\n<p><br></p>\n<p>3. LLM Applications and Best Practices</p>\n<p>[00:44:29] 3.1 Practical LLM Applications and Productivity Gains  </p>\n<p>[00:49:51] 3.2 Effective LLM Usage and Prompting Strategies  </p>\n<p>[00:53:03] 3.3 Security Vulnerabilities in LLM-Generated Code  </p>\n<p><br></p>\n<p>4. Advanced LLM Research and Architecture</p>\n<p>[00:59:13] 4.1 LLM Code Generation Performance and O(1) Labs Experience  </p>\n<p>[01:03:31] 4.2 Adaptation Patterns and Benchmarking Challenges  </p>\n<p>[01:10:10] 4.3 Model Stealing Research and Production LLM Architecture Extraction  </p>\n<p><br></p>\n<p>REFS:</p>\n<p>[00:01:15] Nicholas Carlini\u2019s personal website &amp; research profile (Google DeepMind, ML security) - https://nicholas.carlini.com/</p>\n<p><br></p>\n<p>[00:01:50] CentML AI compute platform for language model workloads - https://centml.ai/</p>\n<p><br></p>\n<p>[00:04:30] Seminal paper on neural network robustness against adversarial examples (Carlini &amp; Wagner, 2016) - https://arxiv.org/abs/1608.04644</p>\n<p><br></p>\n<p>[00:05:20] Computer Fraud and Abuse Act (CFAA) \u2013 primary U.S. federal law on computer hacking liability - https://www.justice.gov/jm/jm-9-48000-computer-fraud</p>\n<p><br></p>\n<p>[00:08:30] Blog post: Emergent chess capabilities in GPT-3.5-turbo-instruct (Nicholas Carlini, Sept 2023) - https://nicholas.carlini.com/writing/2023/chess-llm.html</p>\n<p><br></p>\n<p>[00:16:10] Paper: \u201cSelf-Play Preference Optimization for Language Model Alignment\u201d (Yue Wu et al., 2024) - https://arxiv.org/abs/2405.00675</p>\n<p><br></p>\n<p>[00:18:00] GPT-4 Technical Report: development, capabilities, and calibration analysis - https://arxiv.org/abs/2303.08774</p>\n<p><br></p>\n<p>[00:22:40] Historical shift from descriptive to algebraic chess notation (FIDE) - https://en.wikipedia.org/wiki/Descriptive_notation</p>\n<p><br></p>\n<p>[00:23:55] Analysis of distribution shift in ML (Hendrycks et al.) - https://arxiv.org/abs/2006.16241</p>\n<p><br></p>\n<p>[00:27:40] Nicholas Carlini\u2019s essay \u201cWhy I Attack\u201d (June 2024) \u2013 motivations for security research - https://nicholas.carlini.com/writing/2024/why-i-attack.html</p>\n<p><br></p>\n<p>[00:34:05] Google Project Zero\u2019s 90-day vulnerability disclosure policy - https://googleprojectzero.blogspot.com/p/vulnerability-disclosure-policy.html</p>\n<p><br></p>\n<p>[00:51:15] Evolution of Google search syntax &amp; user behavior (Daniel M. Russell) - https://www.amazon.com/Joy-Search-Google-Master-Information/dp/0262042878</p>\n<p><br></p>\n<p>[01:04:05] Rust\u2019s ownership &amp; borrowing system for memory safety - https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html</p>\n<p><br></p>\n<p>[01:10:05] Paper: \u201cStealing Part of a Production Language Model\u201d (Carlini et al., March 2024) \u2013 extraction attacks on ChatGPT, PaLM-2 - https://arxiv.org/abs/2403.06634</p>\n<p><br></p>\n<p>[01:10:55] First model stealing paper (Tram\u00e8r et al., 2016) \u2013 attacking ML APIs via prediction - https://arxiv.org/abs/1609.02943</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/97560401/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-25%2Fe2cfa06e-9d17-d934-48a1-c88f969a4f47.mp3"
}