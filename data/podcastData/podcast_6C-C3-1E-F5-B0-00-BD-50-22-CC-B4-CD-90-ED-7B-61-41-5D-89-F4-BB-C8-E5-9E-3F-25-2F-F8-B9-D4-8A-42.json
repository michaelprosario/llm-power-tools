{
    "Id": "6C-C3-1E-F5-B0-00-BD-50-22-CC-B4-CD-90-ED-7B-61-41-5D-89-F4-BB-C8-E5-9E-3F-25-2F-F8-B9-D4-8A-42",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "#112 AVOIDING AGI APOCALYPSE - CONNOR LEAHY",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/112-AVOIDING-AGI-APOCALYPSE---CONNOR-LEAHY-e21ji45",
    "Description": "<p>Support us! https://www.patreon.com/mlst \nMLST Discord: https://discord.gg/aNPkGUQtc5\n\nIn this podcast with the legendary Connor Leahy (CEO Conjecture) recorded in Dec 2022, we discuss various topics related to artificial intelligence (AI), including AI alignment, the success of ChatGPT, the potential threats of artificial general intelligence (AGI), and the challenges of balancing research and product development at his company, Conjecture. He emphasizes the importance of empathy, dehumanizing our thinking to avoid anthropomorphic biases, and the value of real-world experiences in learning and personal growth. The conversation also covers the Orthogonality Thesis, AI preferences, the mystery of mode collapse, and the paradox of AI alignment.\n\nConnor Leahy expresses concern about the rapid development of AI and the potential dangers it poses, especially as AI systems become more powerful and integrated into society. He argues that we need a better understanding of AI systems to ensure their safe and beneficial development. The discussion also touches on the concept of &quot;futuristic whack-a-mole,&quot; where futurists predict potential AGI threats, and others try to come up with solutions for those specific scenarios. However, the problem lies in the fact that there could be many more scenarios that neither party can think of, especially when dealing with a system that&#39;s smarter than humans.\n\nhttps://www.linkedin.com/in/connor-j-leahy/https://twitter.com/NPCollapse\n\nInterviewer: Dr. Tim Scarfe (Innovation CTO @ XRAI Glass https://xrai.glass/)\n\nTOC:\nThe success of ChatGPT and its impact on the AI field [00:00:00]\nSubjective experience [00:15:12]\nAI Architectural discussion including RLHF [00:18:04]\nThe paradox of AI alignment and the future of AI in society [00:31:44]\nThe impact of AI on society and politics [00:36:11]\nFuture shock levels and the challenges of predicting the future [00:45:58]\nLong termism and existential risk [00:48:23]\nConsequentialism vs. deontology in rationalism [00:53:39]\nThe Rationalist Community and its Challenges [01:07:37]\nAI Alignment and Conjecture [01:14:15]\nOrthogonality Thesis and AI Preferences [01:17:01]\nChallenges in AI Alignment [01:20:28]\nMechanistic Interpretability in Neural Networks [01:24:54]\nBuilding Cleaner Neural Networks [01:31:36]\nCognitive horizons / The problem with rapid AI development [01:34:52]\nFounding Conjecture and raising funds [01:39:36]\nInefficiencies in the market and seizing opportunities [01:45:38]\nCharisma, authenticity, and leadership in startups [01:52:13]\nAutistic culture and empathy [01:55:26]\nLearning from real-world experiences [02:01:57]\nTechnical empathy and transhumanism [02:07:18]\nMoral status and the limits of empathy [02:15:33]\nAnthropomorphic Thinking and Consequentialism [02:17:42]\nConjecture: Balancing Research and Product Development [02:20:37]\nEpistemology Team at Conjecture [02:31:07]\nInterpretability and Deception in AGI [02:36:23]\nFuturistic whack-a-mole and predicting AGI threats [02:38:27]\n\nRefs:\n1. OpenAI&#39;s ChatGPT: https://chat.openai.com/\n2. The Mystery of Mode Collapse (Article): https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse \n3. The Rationalist Guide to the Galaxy https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795\n5. Alfred Korzybski: https://en.wikipedia.org/wiki/Alfred_Korzybski\n6. Instrumental Convergence: https://en.wikipedia.org/wiki/Instrumental_convergence\n7. Orthogonality Thesis: https://en.wikipedia.org/wiki/Orthogonality_thesis\n8. Brian Tomasik&#39;s Essays on Reducing Suffering: https://reducing-suffering.org/\n9. Epistemological Framing for AI Alignment Research: https://www.lesswrong.com/posts/Y4YHTBziAscS5WPN7/epistemological-framing-for-ai-alignment-research\n10. How to Defeat Mind readers: https://www.alignmentforum.org/posts/EhAbh2pQoAXkm9yor/circumventing-interpretability-how-to-defeat-mind-readers\n11. Society of mind: https://www.amazon.co.uk/Society-Mind-Marvin-Minsky/dp/0671607405</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/67798597/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-3-2%2F2b84c075-edce-9bd9-9a26-800f67c3441d.mp3"
}