{
    "Id": "01-F5-F6-1B-B6-35-A1-90-C5-A9-F4-FF-FB-35-2B-B7-8B-13-FB-E5-44-C2-EA-E5-BA-3C-D3-0E-90-92-6C-B6",
    "ContentSourceId": "03e85e81-f3bb-4c2c-944d-b6722870407b",
    "Title": "283: Getting The Most Out of Data With Gradient Boosting",
    "SourceUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD2875528635.mp3?updated=1723453395",
    "Description": "\n        <p>In this episode of the SuperDataScience Podcast, I chat with one of the key people behind the Python package scikit-learn, Andreas Mueller. You will learn about gradient boosting algorithms, XGBoost, LightGBM and HistGradientBoosting. You will hear Andreas's approach to solving problems, what machine learning algorithms he prefers to apply to a given data science challenge, in which order and why. You will also hear about problems with Kaggle competitions. You will find out the four key questions that Andreas recommends to ask when you have a data challenge in front of you. You will learn about his 95% rule to creating models, and creating success in business enterprises with the help of machine learning. And, finally, you will also learn about the Data Science Institute at Columbia University.</p><p><br></p><p>If you enjoyed this episode, check out show notes, resources, and more at <a href=\"https://www.superdatascience.com/283\">www.superdatascience.com/283</a></p>\n      ",
    "EnclosureUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD2875528635.mp3?updated=1723453395"
}