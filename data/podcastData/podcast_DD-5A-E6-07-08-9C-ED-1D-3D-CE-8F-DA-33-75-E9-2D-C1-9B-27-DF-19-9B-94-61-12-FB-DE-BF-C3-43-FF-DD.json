{
    "Id": "DD-5A-E6-07-08-9C-ED-1D-3D-CE-8F-DA-33-75-E9-2D-C1-9B-27-DF-19-9B-94-61-12-FB-DE-BF-C3-43-FF-DD",
    "ContentSourceId": "03e85e81-f3bb-4c2c-944d-b6722870407b",
    "Title": "687: Generative Deep Learning, with David Foster",
    "SourceUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD3360211251.mp3?updated=1716819079",
    "Description": "\n        <p>Autoencoders, transformers, latent space: Learn the elements of generative AI and hear what data scientist David Foster has to say about the potential for generative AI in music, as well as the role that world models play in blending generative AI with reinforcement learning.<br><br>This episode is brought to you by <a href=\"https://posit.co/\">Posit</a>, the open-source data science company, by <a href=\"https://superdatascience.com/anaconda\">Anaconda</a>, the world's most popular Python distribution, and by <a href=\"https://withfeeling.ai/\">WithFeeling.ai</a>, the company bringing humanity into AI. Interested in sponsoring a SuperDataScience Podcast episode? Visit <a href=\"https://jonkrohn.com/podcast\">JonKrohn.com/podcast</a> for sponsorship information.<br><br>In this episode you will learn:<br>\u2022 Generative modeling vs discriminative modeling [04:21]<br>\u2022 Generative AI for Music [13:12]<br>\u2022 On the threats of AI [23:15]<br>\u2022 Autoencoders Explained [38:36]<br>\u2022 Noise in Generative AI [48:11]<br>\u2022 What CLIP models are (Contrastive Language-Image Pre-training) [54:07]<br>\u2022 What World Models are [1:00:40]<br>\u2022 What a Transformer is [1:11:14]<br>\u2022 How to use transformers for music generation [1:19:50]<br><br>Additional materials: <a href=\"https://www.superdatascience.com/687\">www.superdatascience.com/687</a></p>\n      ",
    "EnclosureUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD3360211251.mp3?updated=1716819079"
}