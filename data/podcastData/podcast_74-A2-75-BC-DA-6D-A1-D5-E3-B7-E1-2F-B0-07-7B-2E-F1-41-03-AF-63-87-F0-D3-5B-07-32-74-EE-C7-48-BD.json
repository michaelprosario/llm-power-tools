{
    "Id": "74-A2-75-BC-DA-6D-A1-D5-E3-B7-E1-2F-B0-07-7B-2E-F1-41-03-AF-63-87-F0-D3-5B-07-32-74-EE-C7-48-BD",
    "ContentSourceId": "03e85e81-f3bb-4c2c-944d-b6722870407b",
    "Title": "670: LLaMA: GPT-3 performance, 10x smaller",
    "SourceUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD2780731966.mp3?updated=1716819144",
    "Description": "\n        <p>How does Meta AI's natural language model, LLaMa compare to the rest? Based on the Chinchilla scaling laws, LLaMa is designed to be smaller but more performant. But how exactly does it achieve this feat? It's all done by training a small model for a longer period of time. Discover how LLaMa compares to its competition, including GPT-3, in this week's episode. <br><br>Additional materials: <a href=\"https://www.superdatascience.com/670\">www.superdatascience.com/670</a><br><br>Interested in sponsoring a SuperDataScience Podcast episode? Visit <a href=\"https://www.jonkrohn.com/podcast\">JonKrohn.com/podcast</a> for sponsorship information.</p>\n      ",
    "EnclosureUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD2780731966.mp3?updated=1716819144"
}