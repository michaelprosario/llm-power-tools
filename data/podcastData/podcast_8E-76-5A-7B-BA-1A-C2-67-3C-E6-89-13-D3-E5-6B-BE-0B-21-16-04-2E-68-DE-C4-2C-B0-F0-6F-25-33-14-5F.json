{
    "Id": "8E-76-5A-7B-BA-1A-C2-67-3C-E6-89-13-D3-E5-6B-BE-0B-21-16-04-2E-68-DE-C4-2C-B0-F0-6F-25-33-14-5F",
    "ContentSourceId": "03e85e81-f3bb-4c2c-944d-b6722870407b",
    "Title": "759: Full Encoder-Decoder Transformers Fully Explained, with Kirill Eremenko",
    "SourceUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD4699941717.mp3?updated=1716818971",
    "Description": "\n        <p>Encoders, cross attention and masking for LLMs: SuperDataScience Founder Kirill Eremenko returns to the SuperDataScience podcast, where he speaks with Jon Krohn about transformer architectures and why they are a new frontier for generative AI. If you\u2019re interested in applying LLMs to your business portfolio, you\u2019ll want to pay close attention to this episode!<br><br>This episode is brought to you by <a href=\"https://www.readytensor.ai/\">Ready Tensor</a>, where innovation meets reproducibility, by <a href=\"https://netsuite.com/superdata\">Oracle NetSuite</a> business software, and by <a href=\"https://hpe.com/ezmeral/chatbots\">Intel and HPE Ezmeral Software Solutions</a>. Interested in sponsoring a SuperDataScience Podcast episode? Visit <a href=\"https://passionfroot.me/superdatascience\">passionfroot.me/superdatascience</a> for sponsorship information.<br><br>In this episode you will learn:<br>\u2022 How decoder-only transformers work [15:51]<br>\u2022 How cross-attention works in transformers [41:05]<br>\u2022 How encoders and decoders work together (an example) [52:46]<br>\u2022 How encoder-only architectures excel at understanding natural language [1:20:34]<br>\u2022 The importance of masking during self-attention [1:27:08]<br><br>Additional materials: <a href=\"https://www.superdatascience.com/759\">www.superdatascience.com/759</a></p>\n      ",
    "EnclosureUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD4699941717.mp3?updated=1716818971"
}