{
    "Id": "9A-16-05-26-01-71-61-80-12-5A-C9-36-19-8F-D2-59-43-F6-46-9C-6A-87-01-6D-51-C2-99-7F-E3-F3-72-B2",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "#037 - Tour De Bayesian with Connor Tann",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/037---Tour-De-Bayesian-with-Connor-Tann-eoq4a6",
    "Description": "<p>Connor Tan is a physicist and senior data scientist working for a multinational energy company where he co-founded and leads a data science team. He holds a first-class degree in experimental and theoretical physics from Cambridge university. With a master's in particle astrophysics. He specializes in the application of machine learning models and Bayesian methods. Today we explore the history, pratical utility, and unique capabilities of Bayesian methods. We also discuss the computational difficulties inherent in Bayesian methods along with modern methods for approximate solutions such as Markov Chain Monte Carlo. Finally, we discuss how Bayesian optimization in the context of automl may one day put Data Scientists like Connor out of work.</p>\n<p><br></p>\n<p>Panel: Dr. Keith Duggar, Alex Stenlake, Dr. Tim Scarfe</p>\n<p><br></p>\n<p>00:00:00 Duggars philisophical ramblings on Bayesianism</p>\n<p>00:05:10 Introduction</p>\n<p>00:07:30 small datasets and prior scientific knowledge</p>\n<p>00:10:37 Bayesian methods are probability theory</p>\n<p>00:14:00 Bayesian methods demand hard computations</p>\n<p>00:15:46 uncertainty can matter more than estimators</p>\n<p>00:19:29 updating or combining knowledge is a key feature</p>\n<p>00:25:39 Frequency or Reasonable Expectation as the Primary Concept&nbsp;</p>\n<p>00:30:02 Gambling and coin flips</p>\n<p>00:37:32 Rev. Thomas Bayes's pool table</p>\n<p>00:40:37 ignorance priors are beautiful yet hard</p>\n<p>00:43:49 connections between common distributions</p>\n<p>00:49:13 A curious Universe, Benford's Law</p>\n<p>00:55:17 choosing priors, a tale of two factories</p>\n<p>01:02:19 integration, the computational Achilles heel</p>\n<p>01:35:25 Bayesian social context in the ML community</p>\n<p>01:10:24 frequentist methods as a first approximation</p>\n<p>01:13:13 driven to Bayesian methods by small sample size</p>\n<p>01:18:46 Bayesian optimization with automl, a job killer?</p>\n<p>01:25:28 different approaches to hyper-parameter optimization</p>\n<p>01:30:18 advice for aspiring Bayesians</p>\n<p>01:33:59 who would connor interview next?</p>\n<p><br></p>\n<p>Connor Tann: https://www.linkedin.com/in/connor-tann-a92906a1/</p>\n<p>https://twitter.com/connossor</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/25022214/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-0-11%2Fdf974633-9a92-ac67-be02-efb5183444a7.mp3"
}