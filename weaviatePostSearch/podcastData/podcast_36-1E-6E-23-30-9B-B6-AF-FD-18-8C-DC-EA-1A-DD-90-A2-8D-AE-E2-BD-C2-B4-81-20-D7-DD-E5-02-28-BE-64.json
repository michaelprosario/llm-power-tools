{
    "Id": "36-1E-6E-23-30-9B-B6-AF-FD-18-8C-DC-EA-1A-DD-90-A2-8D-AE-E2-BD-C2-B4-81-20-D7-DD-E5-02-28-BE-64",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "#036 - Max Welling: Quantum, Manifolds & Symmetries in ML",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/036---Max-Welling-Quantum--Manifolds--Symmetries-in-ML-eogoe8",
    "Description": "<p>Today we had a fantastic conversation with Professor Max Welling, VP of Technology, Qualcomm Technologies Netherlands B.V.&nbsp;</p>\n<p><br></p>\n<p>Max is a strong believer in the power of data and computation and its relevance to artificial intelligence. There is a fundamental blank slate paradgm in machine learning, experience and data alone currently rule the roost. Max wants to build a house of domain knowledge on top of that blank slate. Max thinks there are no predictions without assumptions, no generalization without inductive bias. The bias-variance tradeoff tells us that we need to use additional human knowledge when data is insufficient.</p>\n<p><br></p>\n<p>Max Welling has pioneered many of the most sophistocated inductive priors in DL models developed in recent years, allowing us to use Deep Learning with non-euclidean data i.e. on graphs/topology (a field we now called \"geometric deep learning\") or allowing network architectures to recognise new symmetries in the data for example gauge or SE(3) equivariance. Max has also brought many other concepts from his physics playbook into ML, for example quantum and even Bayesian approaches.&nbsp;</p>\n<p><br></p>\n<p>This is not an episode to miss, it might be our best yet!&nbsp;</p>\n<p><br></p>\n<p>Panel: Dr. Tim Scarfe, Yannic Kilcher, Alex Stenlake</p>\n<p><br></p>\n<p>00:00:00 Show introduction&nbsp;</p>\n<p>00:04:37 Protein Fold from DeepMind -- did it use SE(3) transformer?&nbsp;</p>\n<p>00:09:58 How has machine learning progressed&nbsp;</p>\n<p>00:19:57 Quantum Deformed Neural Networks paper&nbsp;</p>\n<p>00:22:54 Probabilistic Numeric Convolutional Neural Networks paper</p>\n<p>00:27:04 Ilia Karmanov from Qualcomm interview mini segment</p>\n<p>00:32:04 Main Show Intro&nbsp;</p>\n<p>00:35:21 How is Max known in the community?&nbsp;</p>\n<p>00:36:35 How Max nurtures talent, freedom and relationship is key&nbsp;</p>\n<p>00:40:30 Selecting research directions and guidance&nbsp;</p>\n<p>00:43:42 Priors vs experience (bias/variance trade-off)&nbsp;</p>\n<p>00:48:47 Generative models and GPT-3&nbsp;</p>\n<p>00:51:57 Bias/variance trade off -- when do priors hurt us&nbsp;</p>\n<p>00:54:48 Capsule networks&nbsp;</p>\n<p>01:03:09 Which old ideas whould we revive&nbsp;</p>\n<p>01:04:36 Hardware lottery paper&nbsp;</p>\n<p>01:07:50 Greatness can't be planned (Kenneth Stanley reference)&nbsp;</p>\n<p>01:09:10 A new sort of peer review and originality&nbsp;</p>\n<p>01:11:57 Quantum Computing&nbsp;</p>\n<p>01:14:25 Quantum deformed neural networks paper&nbsp;</p>\n<p>01:21:57 Probabalistic numeric convolutional neural networks&nbsp;</p>\n<p>01:26:35 Matrix exponential&nbsp;</p>\n<p>01:28:44 Other ideas from physics i.e. chaos, holography, renormalisation&nbsp;</p>\n<p>01:34:25 Reddit&nbsp;</p>\n<p>01:37:19 Open review system in ML&nbsp;</p>\n<p>01:41:43 Outro&nbsp;</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/24715144/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-0-3%2Fd0f24c69-d34f-c257-6f76-4af6675487f8.mp3"
}