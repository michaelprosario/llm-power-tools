{
    "Id": "A7-E7-4A-4C-09-25-70-E8-1A-04-FF-5D-67-52-0B-01-36-66-1A-6C-E3-6B-10-B5-E4-07-19-04-36-0D-1F-FC",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "#91 - HATTIE ZHOU - Teaching Algorithmic Reasoning via In-context Learning #NeurIPS",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/91---HATTIE-ZHOU---Teaching-Algorithmic-Reasoning-via-In-context-Learning-NeurIPS-e1sgvh9",
    "Description": "<p>Support us! https://www.patreon.com/mlst</p>\n<p><br></p>\n<p>Hattie Zhou, a PhD student at Universit\u00e9 de Montr\u00e9al and Mila, has set out to understand and explain the performance of modern neural networks, believing it a key factor in building better, more trusted models. Having previously worked as a data scientist at Uber, a private equity analyst at Radar Capital, and an economic consultant at Cornerstone Research, she has recently released a paper in collaboration with the Google Brain team, titled \u2018Teaching Algorithmic Reasoning via In-context Learning\u2019. In this work, Hattie identifies and examines four key stages for successfully teaching algorithmic reasoning to large language models (LLMs): formulating algorithms as skills, teaching multiple skills simultaneously, teaching how to combine skills, and teaching how to use skills as tools. Through the application of algorithmic prompting, Hattie has achieved remarkable results, with an order of magnitude error reduction on some tasks compared to the best available baselines. This breakthrough demonstrates algorithmic prompting\u2019s viability as an approach for teaching algorithmic reasoning to LLMs, and may have implications for other tasks requiring similar reasoning capabilities.</p>\n<p><br></p>\n<p>TOC</p>\n<p>[00:00:00] Hattie Zhou</p>\n<p>[00:19:49] Markus Rabe [Google Brain]</p>\n<p><br></p>\n<p>Hattie's Twitter - https://twitter.com/oh_that_hat</p>\n<p>Website - http://hattiezhou.com/</p>\n<p><br></p>\n<p>Teaching Algorithmic Reasoning via In-context Learning [Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi]</p>\n<p>https://arxiv.org/pdf/2211.09066.pdf</p>\n<p><br></p>\n<p>Markus Rabe [Google Brain]:</p>\n<p>https://twitter.com/markusnrabe</p>\n<p>https://research.google/people/106335/</p>\n<p>https://www.linkedin.com/in/markusnrabe</p>\n<p><br></p>\n<p>Autoformalization with Large Language Models [Albert Jiang Charles Edgar Staats Christian Szegedy Markus Rabe Mateja Jamnik Wenda Li Yuhuai Tony Wu]</p>\n<p>https://research.google/pubs/pub51691/</p>\n<p><br></p>\n<p>Discord: https://discord.gg/aNPkGUQtc5</p>\n<p>YT: https://youtu.be/80i6D2TJdQ4</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/62471145/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-20%2F583c8109-d89f-19c1-8546-79e6540a4469.mp3"
}