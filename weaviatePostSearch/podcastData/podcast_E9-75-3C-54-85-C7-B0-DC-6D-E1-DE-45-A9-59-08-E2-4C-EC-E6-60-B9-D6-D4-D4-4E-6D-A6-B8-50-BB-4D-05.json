{
    "Id": "E9-75-3C-54-85-C7-B0-DC-6D-E1-DE-45-A9-59-08-E2-4C-EC-E6-60-B9-D6-D4-D4-4E-6D-A6-B8-50-BB-4D-05",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "ICLR 2020: Yoshua Bengio and the Nature of Consciousness",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/ICLR-2020-Yoshua-Bengio-and-the-Nature-of-Consciousness-eeecct",
    "Description": "<p>In this episode of Machine Learning Street Talk, Tim Scarfe, Connor Shorten and Yannic Kilcher react to Yoshua Bengio\u2019s ICLR 2020 Keynote \u201cDeep Learning Priors Associated with Conscious Processing\u201d. Bengio takes on many future directions for research in Deep Learning such as the role of attention in consciousness, sparse factor graphs and causality, and the study of systematic generalization. Bengio also presents big ideas in Intelligence that border on the line of philosophy and practical machine learning. This includes ideas such as consciousness in machines and System 1 and System 2 thinking, as described in Daniel Kahneman\u2019s book \u201cThinking Fast and Slow\u201d. Similar to Yann LeCun\u2019s half of the 2020 ICLR keynote, this talk takes on many challenging ideas and hopefully this video helps you get a better understanding of some of them! Thanks for watching!&nbsp;</p>\n<p>Please Subscribe for more videos!</p>\n<p>Paper Links:</p>\n<p>Link to Talk: https://iclr.cc/virtual_2020/speaker_7.html</p>\n<p>The Consciousness Prior: https://arxiv.org/abs/1709.08568</p>\n<p>Thinking Fast and Slow: https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555</p>\n<p>Systematic Generalization: https://arxiv.org/abs/1811.12889</p>\n<p>CLOSURE: Assessing Systematic Generalization of CLEVR Models: https://arxiv.org/abs/1912.05783</p>\n<p>Neural Module Networks: https://arxiv.org/abs/1511.02799</p>\n<p>Experience Grounds Language: https://arxiv.org/pdf/2004.10151.pdf</p>\n<p>Benchmarking Graph Neural Networks: https://arxiv.org/pdf/2003.00982.pdf</p>\n<p>On the Measure of Intelligence: https://arxiv.org/abs/1911.01547</p>\n<p>Please check out our individual channels as well!</p>\n<p>Machine Learning Dojo with Tim Scarfe: https://www.youtube.com/channel/UCXvHuBMbgJw67i5vrMBBobA</p>\n<p>Yannic Kilcher: https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfe</p>\n<p>Henry AI Labs: https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw</p>\n<p>00:00:00 Tim and Yannics takes</p>\n<p>00:01:37 Intro to Bengio</p>\n<p>00:03:13 System 2, language and Chomsky</p>\n<p>00:05:58 Cristof Koch on conciousness</p>\n<p>00:07:25 Francois Chollet on intelligence and consciousness</p>\n<p>00:09:29 Meditation and Sam Harris on consciousness</p>\n<p>00:11:35 Connor Intro</p>\n<p>00:13:20 Show Main Intro</p>\n<p>00:17:55 Priors associated with Conscious Processing</p>\n<p>00:26:25 System 1 / System 2</p>\n<p>00:42:47 Implicit and Verbalized Knowledge [DONT MISS THIS!]</p>\n<p>01:08:24 Inductive Priors for DL 2.0</p>\n<p>01:27:20 Systematic Generalization</p>\n<p>01:37:53 Contrast with the Symbolic AI Program</p>\n<p>01:54:55 Attention</p>\n<p>02:00:25 From Attention to Consciousness</p>\n<p>02:05:31 Thoughts, Consciousness, Language</p>\n<p>02:06:55 Sparse Factor graph</p>\n<p>02:10:52 Sparse Change in Abstract Latent Space</p>\n<p>02:15:10 Discovering Cause and Effect</p>\n<p>02:20:00 Factorize the joint distribution</p>\n<p>02:22:30 RIMS: Modular Computation</p>\n<p>02:24:30 Conclusion</p>\n<p>#machinelearning #deeplearning</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/14151517/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-4-22%2F75773970-48000-2-e0411642470b9.mp3"
}