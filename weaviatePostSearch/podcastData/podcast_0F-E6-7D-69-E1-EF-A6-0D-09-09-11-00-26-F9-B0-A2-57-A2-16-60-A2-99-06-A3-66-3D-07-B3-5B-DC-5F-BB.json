{
    "Id": "0F-E6-7D-69-E1-EF-A6-0D-09-09-11-00-26-F9-B0-A2-57-A2-16-60-A2-99-06-A3-66-3D-07-B3-5B-DC-5F-BB",
    "ContentSourceId": "03e85e81-f3bb-4c2c-944d-b6722870407b",
    "Title": "905: Why RAG Makes LLMs Less Safe (And How to Fix It), with Bloomberg\u2019s Dr. Sebastian Gehrmann",
    "SourceUrl": null,
    "Description": "RAG LLMs are not safer: Sebastian Gehrmann speaks to Jon Krohn about his latest research into how retrieval-augmented generation (RAG) actually makes LLMs less safe, the three \u2018H\u2019s for gauging the effectivity and value of a RAG, and the custom guardrails and procedures we need to use to ensure our RAG is fit-for-purpose and secure. This is a great episode for anyone who wants to know how to work with RAG in the context of LLMs, as you\u2019ll hear how to select the best model for purpose, useful approaches and taxonomies to keep your projects secure, and which models he finds safest when RAG is applied.\n\n\n\nAdditional materials: \u2060\u2060\u2060\u2060\u2060\u2060www.superdatascience.com/905\u2060\u2060\n\n\n\nThis episode is brought to you\u2060 by,  \u2060\u2060\u2060Adverity, the conversational analytics platform\u2060\u2060\u2060 and by the \u2060\u2060\u2060Dell AI Factory with NVIDIA\u2060\u2060\u2060.\n\n\n\nInterested in sponsoring a SuperDataScience Podcast episode? Email natalie@superdatascience.com for sponsorship information.\n\n\n\nIn this episode you will learn: \n\n\n  (03:28) Findings from the paper \u201cRAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models\u201d\n\n  (09:35) What attack surfaces are in the context of AI\n\n  (38:51) Small versus large models with RAG\n\n  (46:27) How to select an LLM with safety in mind",
    "EnclosureUrl": "https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD5498286974.mp3?updated=1752067464"
}