{
    "Id": "6E-EC-23-03-AF-DB-92-D5-09-8C-F0-8A-D2-5C-73-C7-9D-E3-50-73-8C-78-99-E8-28-2C-33-5E-26-BA-EB-5B",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "Sakana AI - Chris Lu, Robert Tjarko Lange, Cong Lu",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Sakana-AI---Chris-Lu--Robert-Tjarko-Lange--Cong-Lu-e2vih1p",
    "Description": "<p>We speak with Sakana AI, who are building nature-inspired methods that could fundamentally transform how we develop AI systems.</p><p><br></p><p>The guests include Chris Lu, a researcher who recently completed his DPhil at Oxford University under Prof. Jakob Foerster&#39;s supervision, where he focused on meta-learning and multi-agent systems. Chris is the first author of the DiscoPOP paper, which demonstrates how language models can discover and design better training algorithms. Also joining is Robert Tjarko Lange, a founding member of Sakana AI who specializes in evolutionary algorithms and large language models. Robert leads research at the intersection of evolutionary computation and foundation models, and is completing his PhD at TU Berlin on evolutionary meta-learning. The discussion also features Cong Lu, currently a Research Scientist at Google DeepMind&#39;s Open-Endedness team, who previously helped develop The AI Scientist and Intelligent Go-Explore.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!</p><p>https://centml.ai/pricing/</p><p><br></p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p><br></p><p>* DiscoPOP - A framework where language models discover their own optimization algorithms</p><p>* EvoLLM - Using language models as evolution strategies for optimization</p><p>The AI Scientist - A fully automated system that conducts scientific research end-to-end</p><p>* Neural Attention Memory Models (NAMMs) - Evolved memory systems that make transformers both faster and more accurate</p><p><br></p><p>TRANSCRIPT + REFS:</p><p>https://www.dropbox.com/scl/fi/gflcyvnujp8cl7zlv3v9d/Sakana.pdf?rlkey=woaoo82943170jd4yyi2he71c&amp;dl=0</p><p><br></p><p>Robert Tjarko Lange</p><p>https://roberttlange.com/</p><p>Chris Lu</p><p>https://chrislu.page/</p><p>Cong Lu</p><p>https://www.conglu.co.uk/</p><p>Sakana</p><p>https://sakana.ai/blog/</p><p><br></p><p>TOC:</p><p>1. LLMs for Algorithm Generation and Optimization</p><p> [00:00:00] 1.1 LLMs generating algorithms for training other LLMs</p><p>   [00:04:00] 1.2 Evolutionary black-box optim using neural network loss parameterization</p><p>   [00:11:50] 1.3 DiscoPOP: Non-convex loss function for noisy data</p><p>   [00:20:45] 1.4 External entropy Injection for preventing Model collapse</p><p>   [00:26:25] 1.5 LLMs for black-box optimization using abstract numerical sequences</p><p><br></p><p>2. Model Learning and Generalization</p><p>   [00:31:05] 2.1 Fine-tuning on teacher algorithm trajectories</p><p>   [00:31:30] 2.2 Transformers learning gradient descent</p><p>   [00:33:00] 2.3 LLM tokenization biases towards specific numbers</p><p>   [00:34:50] 2.4 LLMs as evolution strategies for black box optimization</p><p>   [00:38:05] 2.5 DiscoPOP: LLMs discovering novel optimization algorithms</p><p><br></p><p>3. AI Agents and System Architectures</p><p>   [00:51:30] 3.1 ARC challenge: Induction vs. transformer approaches</p><p>   [00:54:35] 3.2 LangChain / modular agent components</p><p>   [00:57:50] 3.3 Debate improves LLM truthfulness</p><p>   [01:00:55] 3.4 Time limits controlling AI agent systems</p><p>   [01:03:00] 3.5 Gemini: Million-token context enables flatter hierarchies</p><p>   [01:04:05] 3.6 Agents follow own interest gradients</p><p>   [01:09:50] 3.7 Go-Explore algorithm: archive-based exploration</p><p>   [01:11:05] 3.8 Foundation models for interesting state discovery</p><p>   [01:13:00] 3.9 LLMs leverage prior game knowledge</p><p><br></p><p>4. AI for Scientific Discovery and Human Alignment</p><p>   [01:17:45] 4.1 Encoding Alignment &amp; Aesthetics via Reward Functions</p><p>   [01:20:00] 4.2 AI Scientist: Automated Open-Ended Scientific Discovery</p><p>   [01:24:15] 4.3 DiscoPOP: LLM for Preference Optimization Algorithms</p><p>   [01:28:30] 4.4 Balancing AI Knowledge with Human Understanding</p><p>   [01:33:55] 4.5 AI-Driven Conferences and Paper Review</p><p><br></p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/99222009/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-1%2F69665050-bc4f-a25b-5de2-11f53c0f4735.mp3"
}