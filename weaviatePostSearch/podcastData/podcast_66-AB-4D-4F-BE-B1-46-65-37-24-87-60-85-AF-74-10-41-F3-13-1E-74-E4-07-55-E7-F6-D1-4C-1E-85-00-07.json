{
    "Id": "66-AB-4D-4F-BE-B1-46-65-37-24-87-60-85-AF-74-10-41-F3-13-1E-74-E4-07-55-E7-F6-D1-4C-1E-85-00-07",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "#81 JULIAN TOGELIUS, Prof. KEN STANLEY - AGI, Games, Diversity & Creativity [UNPLUGGED]",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/81-JULIAN-TOGELIUS--Prof--KEN-STANLEY---AGI--Games--Diversity--Creativity-UNPLUGGED-e1r1jhu",
    "Description": "<p>Support us (and please rate on podcast app)</p>\n<p>https://www.patreon.com/mlst&nbsp;</p>\n<p><br></p>\n<p>In this show tonight with Prof. Julian Togelius (NYU) and Prof. Ken Stanley we discuss open-endedness, AGI, game AI and reinforcement learning. &nbsp;</p>\n<p><br></p>\n<p>[Prof Julian Togelius]</p>\n<p>https://engineering.nyu.edu/faculty/julian-togelius</p>\n<p>https://twitter.com/togelius</p>\n<p><br></p>\n<p>[Prof Ken Stanley]</p>\n<p>https://www.cs.ucf.edu/~kstanley/</p>\n<p>https://twitter.com/kenneth0stanley</p>\n<p><br></p>\n<p>TOC:</p>\n<p>[00:00:00] Introduction</p>\n<p>[00:01:07] AI and computer games</p>\n<p>[00:12:23] Intelligence</p>\n<p>[00:21:27] Intelligence Explosion</p>\n<p>[00:25:37] What should we be aspiring towards?</p>\n<p>[00:29:14] Should AI contribute to culture?</p>\n<p>[00:32:12] On creativity and open-endedness</p>\n<p>[00:36:11] RL overfitting</p>\n<p>[00:44:02] Diversity preservation</p>\n<p>[00:51:18] Empiricism vs rationalism , in gradient descent the data pushes you around</p>\n<p>[00:55:49] Creativity and interestingness (does complexity / information increase)</p>\n<p>[01:03:20] What does a population give us?</p>\n<p>[01:05:58] Emergence / generalisation snobbery</p>\n<p><br></p>\n<p>References;</p>\n<p>[Hutter/Legg] Universal Intelligence: A Definition of Machine Intelligence</p>\n<p>https://arxiv.org/abs/0712.3329</p>\n<p><br></p>\n<p>https://en.wikipedia.org/wiki/Artificial_general_intelligence</p>\n<p>https://en.wikipedia.org/wiki/I._J._Good</p>\n<p>https://en.wikipedia.org/wiki/G%C3%B6del_machine</p>\n<p><br></p>\n<p>[Chollet] Impossibility of intelligence explosion</p>\n<p>https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec</p>\n<p><br></p>\n<p>[Alex Irpan] - RL is hard</p>\n<p>https://www.alexirpan.com/2018/02/14/rl-hard.html</p>\n<p>https://nethackchallenge.com/</p>\n<p>Map elites</p>\n<p>https://arxiv.org/abs/1504.04909</p>\n<p><br></p>\n<p>Covariance Matrix Adaptation for the Rapid Illumination of Behavior Space</p>\n<p>https://arxiv.org/abs/1912.02400</p>\n<p><br></p>\n<p>[Stanley] - Why greatness cannot be planned</p>\n<p>https://www.amazon.com/Why-Greatness-Cannot-Planned-Objective/dp/3319155237</p>\n<p><br></p>\n<p>[Lehman/Stanley] Abandoning Objectives: Evolution through the Search for Novelty Alone</p>\n<p>https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/lehman_ecj11.pdf</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/60918782/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-20%2F02b7ed72-9d32-982d-f87c-b46408a6410d.mp3"
}