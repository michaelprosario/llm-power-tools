{
    "Id": "EB-A1-75-00-E0-37-4F-7D-5C-0A-89-98-B7-70-56-71-CC-DB-B6-4E-56-A2-BB-9B-6A-30-CC-C8-07-77-AB-CE",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "AI Alignment & AGI Fire Alarm - Connor Leahy",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/AI-Alignment--AGI-Fire-Alarm---Connor-Leahy-elsod2",
    "Description": "<p>This week Dr. Tim Scarfe, Alex Stenlake and Yannic Kilcher speak with AGI and AI alignment specialist Connor Leahy a machine learning engineer from Aleph Alpha and founder of EleutherAI.</p>\n<p><br></p>\n<p>Connor believes that AI alignment is philosophy with a deadline and that we are on the precipice, the stakes are astronomical. AI is important, and it will go wrong by default. Connor thinks that the singularity or intelligence explosion is near. Connor says that AGI is like climate change but worse, even harder problems, even shorter deadline and even worse consequences for the future. These problems are hard, and nobody knows what to do about them.</p>\n<p><br></p>\n<p>00:00:00 Introduction to AI alignment and AGI fire alarm&nbsp;</p>\n<p>00:15:16 Main Show Intro&nbsp;</p>\n<p>00:18:38 Different schools of thought on AI safety&nbsp;</p>\n<p>00:24:03 What is intelligence?&nbsp;</p>\n<p>00:25:48 AI Alignment&nbsp;</p>\n<p>00:27:39 Humans dont have a coherent utility function&nbsp;</p>\n<p>00:28:13 Newcomb's paradox and advanced decision problems&nbsp;</p>\n<p>00:34:01 Incentives and behavioural economics&nbsp;</p>\n<p>00:37:19 Prisoner's dilemma&nbsp;</p>\n<p>00:40:24 Ayn Rand and game theory in politics and business&nbsp;</p>\n<p>00:44:04 Instrumental convergence and orthogonality thesis&nbsp;</p>\n<p>00:46:14 Utility functions and the Stop button problem&nbsp;</p>\n<p>00:55:24 AI corrigibality - self alignment&nbsp;</p>\n<p>00:56:16 Decision theory and stability / wireheading / robust delegation&nbsp;</p>\n<p>00:59:30 Stop button problem&nbsp;</p>\n<p>01:00:40 Making the world a better place&nbsp;</p>\n<p>01:03:43 Is intelligence a search problem?&nbsp;</p>\n<p>01:04:39 Mesa optimisation / humans are misaligned AI&nbsp;</p>\n<p>01:06:04 Inner vs outer alignment / faulty reward functions&nbsp;</p>\n<p>01:07:31 Large corporations are intelligent and have no stop function&nbsp;</p>\n<p>01:10:21 Dutch booking / what is rationality / decision theory&nbsp;</p>\n<p>01:16:32 Understanding very powerful AIs&nbsp;</p>\n<p>01:18:03 Kolmogorov complexity&nbsp;</p>\n<p>01:19:52 GPT-3 - is it intelligent, are humans even intelligent?&nbsp;</p>\n<p>01:28:40 Scaling hypothesis&nbsp;</p>\n<p>01:29:30 Connor thought DL was dead in 2017&nbsp;</p>\n<p>01:37:54 Why is GPT-3 as intelligent as a human&nbsp;</p>\n<p>01:44:43 Jeff Hawkins on intelligence as compression and the great lookup table&nbsp;</p>\n<p>01:50:28 AI ethics related to AI alignment?&nbsp;</p>\n<p>01:53:26 Interpretability&nbsp;</p>\n<p>01:56:27 Regulation&nbsp;</p>\n<p>01:57:54 Intelligence explosion&nbsp;</p>\n<p><br></p>\n<p><br></p>\n<p>Discord: https://discord.com/invite/vtRgjbM</p>\n<p>EleutherAI: https://www.eleuther.ai</p>\n<p>Twitter: https://twitter.com/npcollapse</p>\n<p>LinkedIn: https://www.linkedin.com/in/connor-j-leahy/</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/21962594/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-10-1%2Fa6531d59-0540-6bc8-bdc1-f11b1f081ab4.mp3"
}