{
    "Id": "BA-B1-C2-E3-6D-EA-8A-05-0B-95-56-3C-4E-67-41-C1-F7-26-E4-16-3B-D0-C2-24-58-C5-E2-64-68-27-DF-88",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "What\u2019s the Magic Word? A Control Theory of LLM Prompting.",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Whats-the-Magic-Word--A-Control-Theory-of-LLM-Prompting-e2khs2t",
    "Description": "<p>These two scientists have mapped out the insides or \u201creachable space\u201d of a language model using control theory, what they discovered was extremely surprising. </p>\n<p><br></p>\n<p>Please support us on Patreon to get access to the private Discord server, bi-weekly calls, early access and ad-free listening.</p>\n<p>https://patreon.com/mlst</p>\n<p><br></p>\n<p>YT version: https://youtu.be/Bpgloy1dDn0</p>\n<p><br></p>\n<p>Aman Bhargava from Caltech and Cameron Witkowski from the University of Toronto to discuss their groundbreaking paper, \u201cWhat\u2019s the Magic Word? A Control Theory of LLM Prompting.\u201d (the main theorem on self-attention controllability was developed in collaboration with Dr. Shi-Zhuo Looi from Caltech).</p>\n<p><br></p>\n<p>They frame LLM systems as discrete stochastic dynamical systems. This means they look at LLMs in a structured way, similar to how we analyze control systems in engineering. They explore the \u201creachable set\u201d of outputs for an LLM. Essentially, this is the range of possible outputs the model can generate from a given starting point when influenced by different prompts. The research highlights that prompt engineering, or optimizing the input tokens, can significantly influence LLM outputs. They show that even short prompts can drastically alter the likelihood of specific outputs. Aman and Cameron\u2019s work might be a boon for understanding and improving LLMs. They suggest that a deeper exploration of control theory concepts could lead to more reliable and capable language models.</p>\n<p><br></p>\n<p>We dropped an additional, more technical video on the research on our Twitter account here: https://x.com/MLStreetTalk/status/1795093759471890606</p>\n<p><br></p>\n<p>Additional 20 minutes of unreleased footage on our Patreon here: https://www.patreon.com/posts/whats-magic-word-104922629</p>\n<p><br></p>\n<p>What&#39;s the Magic Word? A Control Theory of LLM Prompting (Aman Bhargava, Cameron Witkowski, Manav Shah, Matt Thomson)</p>\n<p>https://arxiv.org/abs/2310.04444</p>\n<p><br></p>\n<p>LLM Control Theory Seminar (April 2024)</p>\n<p>https://www.youtube.com/watch?v=9QtS9sVBFM0</p>\n<p><br></p>\n<p>Society for the pursuit of AGI (Cameron founded it)</p>\n<p>https://agisociety.mydurable.com/</p>\n<p><br></p>\n<p>Roger Federer demo</p>\n<p>http://conway.languagegame.io/inference</p>\n<p><br></p>\n<p>Neural Cellular Automata, Active Inference, and the Mystery of Biological Computation (Aman)</p>\n<p>https://aman-bhargava.com/ai/neuro/neuromorphic/2024/03/25/nca-do-active-inference.html </p>\n<p><br></p>\n<p>Aman and Cameron also want to thank Dr. Shi-Zhuo Looi and Prof. Matt Thomson from from Caltech for help and advice on their research. (https://thomsonlab.caltech.edu/ and https://pma.caltech.edu/people/looi-shi-zhuo)</p>\n<p><br></p>\n<p>https://x.com/ABhargava2000</p>\n<p>https://x.com/witkowski_cam</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/87666205/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-5-5%2Fedcc7f6f-d812-d535-6844-d68532b9a96b.mp3"
}