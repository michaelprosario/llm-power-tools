{
    "Id": "EE-CA-29-FB-05-9B-D4-00-69-06-E3-15-64-A0-E4-A0-29-F3-A9-24-23-62-E6-A7-18-A9-0F-4E-40-6E-07-D1",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "Clement Bonnet - Can Latent Program Networks Solve Abstract Reasoning?",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Clement-Bonnet---Can-Latent-Program-Networks-Solve-Abstract-Reasoning-e2v41og",
    "Description": "<p>Clement Bonnet discusses his novel approach to the ARC (Abstraction and Reasoning Corpus) challenge. Unlike approaches that rely on fine-tuning LLMs or generating samples at inference time, Clement&#39;s method encodes input-output pairs into a latent space, optimizes this representation with a search algorithm, and decodes outputs for new inputs. This end-to-end architecture uses a VAE loss, including reconstruction and prior losses. </p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!</p><p>https://centml.ai/pricing/</p><p><br></p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>TRANSCRIPT + RESEARCH OVERVIEW:</p><p>https://www.dropbox.com/scl/fi/j7m0gaz1126y594gswtma/CLEMMLST.pdf?rlkey=y5qvwq2er5nchbcibm07rcfpq&amp;dl=0</p><p><br></p><p>Clem and Matthew-</p><p>https://www.linkedin.com/in/clement-bonnet16/</p><p>https://github.com/clement-bonnet</p><p>https://mvmacfarlane.github.io/</p><p><br></p><p>TOC</p><p>1. LPN Fundamentals</p><p> [00:00:00] 1.1 Introduction to ARC Benchmark and LPN Overview</p><p>   [00:05:05] 1.2 Neural Networks&#39; Challenges with ARC and Program Synthesis</p><p>   [00:06:55] 1.3 Induction vs Transduction in Machine Learning</p><p><br></p><p>2. LPN Architecture and Latent Space</p><p>   [00:11:50] 2.1 LPN Architecture and Latent Space Implementation</p><p>   [00:16:25] 2.2 LPN Latent Space Encoding and VAE Architecture</p><p>   [00:20:25] 2.3 Gradient-Based Search Training Strategy</p><p>   [00:23:39] 2.4 LPN Model Architecture and Implementation Details</p><p><br></p><p>3. Implementation and Scaling</p><p>   [00:27:34] 3.1 Training Data Generation and re-ARC Framework</p><p>   [00:31:28] 3.2 Limitations of Latent Space and Multi-Thread Search</p><p>   [00:34:43] 3.3 Program Composition and Computational Graph Architecture</p><p><br></p><p>4. Advanced Concepts and Future Directions</p><p>   [00:45:09] 4.1 AI Creativity and Program Synthesis Approaches</p><p>   [00:49:47] 4.2 Scaling and Interpretability in Latent Space Models</p><p><br></p><p>REFS</p><p>[00:00:05] ARC benchmark, Chollet</p><p>https://arxiv.org/abs/2412.04604</p><p><br></p><p>[00:02:10] Latent Program Spaces, Bonnet, Macfarlane</p><p>https://arxiv.org/abs/2411.08706</p><p><br></p><p>[00:07:45] Kevin Ellis work on program generation</p><p>https://www.cs.cornell.edu/~ellisk/</p><p><br></p><p>[00:08:45] Induction vs transduction in abstract reasoning, Li et al.</p><p>https://arxiv.org/abs/2411.02272</p><p><br></p><p>[00:17:40] VAEs, Kingma, Welling</p><p>https://arxiv.org/abs/1312.6114</p><p><br></p><p>[00:27:50] re-ARC, Hodel</p><p>https://github.com/michaelhodel/re-arc</p><p><br></p><p>[00:29:40] Grid size in ARC tasks, Chollet</p><p>https://github.com/fchollet/ARC-AGI</p><p><br></p><p>[00:33:00] Critique of deep learning, Marcus</p><p>https://arxiv.org/vc/arxiv/papers/2002/2002.06177v1.pdf</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/98747600/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-19%2F65f76eb2-6635-fdd1-17ed-23a2f70eb94a.mp3"
}