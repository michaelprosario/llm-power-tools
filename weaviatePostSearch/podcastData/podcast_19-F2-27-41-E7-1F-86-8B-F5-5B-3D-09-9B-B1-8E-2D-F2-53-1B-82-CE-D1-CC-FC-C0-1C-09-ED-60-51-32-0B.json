{
    "Id": "19-F2-27-41-E7-1F-86-8B-F5-5B-3D-09-9B-B1-8E-2D-F2-53-1B-82-CE-D1-CC-FC-C0-1C-09-ED-60-51-32-0B",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "Sayash Kapoor - How seriously should we take AI X-risk? (ICML 1/13)",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Sayash-Kapoor---How-seriously-should-we-take-AI-X-risk--ICML-113-e2mhuoc",
    "Description": "<p>How seriously should governments take the threat of existential risk from AI, given the lack of consensus among researchers? On the one hand, existential risks (x-risks) are necessarily somewhat speculative: by the time there is concrete evidence, it may be too late. On the other hand, governments must prioritize \u2014 after all, they don\u2019t worry too much about x-risk from alien invasions.</p>\n<p><br></p>\n<p>MLST is sponsored by Brave:</p>\n<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at brave.com/api. </p>\n<p><br></p>\n<p>Sayash Kapoor is a computer science Ph.D. candidate at Princeton University&#39;s Center for Information Technology Policy. His research focuses on the societal impact of AI. Kapoor has previously worked on AI in both industry and academia, with experience at Facebook, Columbia University, and EPFL Switzerland. He is a recipient of a best paper award at ACM FAccT and an impact recognition award at ACM CSCW. Notably, Kapoor was included in TIME&#39;s inaugural list of the 100 most influential people in AI.</p>\n<p><br></p>\n<p>Sayash Kapoor</p>\n<p>https://x.com/sayashk</p>\n<p>https://www.cs.princeton.edu/~sayashk/</p>\n<p><br></p>\n<p>Arvind Narayanan (other half of the AI Snake Oil duo)</p>\n<p>https://x.com/random_walker</p>\n<p><br></p>\n<p>AI existential risk probabilities are too unreliable to inform policy</p>\n<p>https://www.aisnakeoil.com/p/ai-existential-risk-probabilities</p>\n<p><br></p>\n<p>Pre-order AI Snake Oil Book</p>\n<p>https://amzn.to/4fq2HGb</p>\n<p><br></p>\n<p>AI Snake Oil blog</p>\n<p>https://www.aisnakeoil.com/</p>\n<p><br></p>\n<p>AI Agents That Matter</p>\n<p>https://arxiv.org/abs/2407.01502</p>\n<p><br></p>\n<p>Shortcut learning in deep neural networks</p>\n<p>https://www.semanticscholar.org/paper/Shortcut-learning-in-deep-neural-networks-Geirhos-Jacobsen/1b04936c2599e59b120f743fbb30df2eed3fd782</p>\n<p><br></p>\n<p>77% Of Employees Report AI Has Increased Workloads And Hampered Productivity, Study Finds</p>\n<p>https://www.forbes.com/sites/bryanrobinson/2024/07/23/employees-report-ai-increased-workload/</p>\n<p><br></p>\n<p>TOC:</p>\n<p>00:00:00 Intro</p>\n<p>00:01:57 How seriously should we take Xrisk threat?</p>\n<p>00:02:55 Risk too unrealiable to inform policy</p>\n<p>00:10:20 Overinflated risks</p>\n<p>00:12:05 Perils of utility maximisation</p>\n<p>00:13:55 Scaling vs airplane speeds</p>\n<p>00:17:31 Shift to smaller models?</p>\n<p>00:19:08 Commercial LLM ecosystem</p>\n<p>00:22:10 Synthetic data</p>\n<p>00:24:09 Is AI complexifying our jobs?</p>\n<p>00:25:50 Does ChatGPT make us dumber or smarter?</p>\n<p>00:26:55 Are AI Agents overhyped?</p>\n<p>00:28:12 Simple vs complex baselines</p>\n<p>00:30:00 Cost tradeoff in agent design</p>\n<p>00:32:30 Model eval vs downastream perf</p>\n<p>00:36:49 Shortcuts in metrics</p>\n<p>00:40:09 Standardisation of agent evals</p>\n<p>00:41:21 Humans in the loop</p>\n<p>00:43:54 Levels of agent generality</p>\n<p>00:47:25 ARC challenge</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/89766092/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-6-28%2Fc7d57c66-93af-1ac3-0a5c-b62ccdcdd61b.mp3"
}