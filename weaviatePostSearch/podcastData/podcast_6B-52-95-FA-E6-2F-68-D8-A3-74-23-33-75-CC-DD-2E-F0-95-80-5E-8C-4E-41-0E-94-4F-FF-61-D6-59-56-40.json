{
    "Id": "6B-52-95-FA-E6-2F-68-D8-A3-74-23-33-75-CC-DD-2E-F0-95-80-5E-8C-4E-41-0E-94-4F-FF-61-D6-59-56-40",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "How Do AI Models Actually Think? - Laura Ruis",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/How-Do-AI-Models-Actually-Think----Laura-Ruis-e2tn9l9",
    "Description": "<p>Laura Ruis, a PhD student at University College London and researcher at Cohere, explains her groundbreaking research into how large language models (LLMs) perform reasoning tasks, the fundamental mechanisms underlying LLM reasoning capabilities, and whether these models primarily rely on retrieval or develop procedural knowledge.</p>\n<p><br></p>\n<p>SPONSOR MESSAGES:</p>\n<p>***</p>\n<p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. </p>\n<p>https://centml.ai/pricing/</p>\n<p><br></p>\n<p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? </p>\n<p><br></p>\n<p>Goto https://tufalabs.ai/</p>\n<p>***</p>\n<p><br></p>\n<p>TOC</p>\n<p><br></p>\n<p>1. LLM Foundations and Learning</p>\n<p> 1.1 Scale and Learning in Language Models [00:00:00]</p>\n<p>   1.2 Procedural Knowledge vs Fact Retrieval [00:03:40]</p>\n<p>   1.3 Influence Functions and Model Analysis [00:07:40]</p>\n<p>   1.4 Role of Code in LLM Reasoning [00:11:10]</p>\n<p>   1.5 Semantic Understanding and Physical Grounding [00:19:30]</p>\n<p><br></p>\n<p>2. Reasoning Architectures and Measurement</p>\n<p>   2.1 Measuring Understanding and Reasoning in Language Models [00:23:10]</p>\n<p>   2.2 Formal vs Approximate Reasoning and Model Creativity [00:26:40]</p>\n<p>   2.3 Symbolic vs Subsymbolic Computation Debate [00:34:10]</p>\n<p>   2.4 Neural Network Architectures and Tensor Product Representations [00:40:50]</p>\n<p><br></p>\n<p>3. AI Agency and Risk Assessment</p>\n<p>   3.1 Agency and Goal-Directed Behavior in Language Models [00:45:10]</p>\n<p>   3.2 Defining and Measuring Agency in AI Systems [00:49:50]</p>\n<p>   3.3 Core Knowledge Systems and Agency Detection [00:54:40]</p>\n<p>   3.4 Language Models as Agent Models and Simulator Theory [01:03:20]</p>\n<p>   3.5 AI Safety and Societal Control Mechanisms [01:07:10]</p>\n<p>   3.6 Evolution of AI Capabilities and Emergent Risks [01:14:20]</p>\n<p><br></p>\n<p>REFS:</p>\n<p>[00:01:10] Procedural Knowledge in Pretraining &amp; LLM Reasoning</p>\n<p>Ruis et al., 2024</p>\n<p>https://arxiv.org/abs/2411.12580</p>\n<p><br></p>\n<p>[00:03:50] EK-FAC Influence Functions in Large LMs</p>\n<p>Grosse et al., 2023</p>\n<p>https://arxiv.org/abs/2308.03296</p>\n<p><br></p>\n<p>[00:13:05] Surfaces and Essences: Analogy as the Core of Cognition</p>\n<p>Hofstadter &amp; Sander</p>\n<p>https://www.amazon.com/Surfaces-Essences-Analogy-Fuel-Thinking/dp/0465018475</p>\n<p><br></p>\n<p>[00:13:45] Wittgenstein on Language Games</p>\n<p>https://plato.stanford.edu/entries/wittgenstein/</p>\n<p><br></p>\n<p>[00:14:30] Montague Semantics for Natural Language</p>\n<p>https://plato.stanford.edu/entries/montague-semantics/</p>\n<p><br></p>\n<p>[00:19:35] The Chinese Room Argument</p>\n<p>David Cole</p>\n<p>https://plato.stanford.edu/entries/chinese-room/</p>\n<p><br></p>\n<p>[00:19:55] ARC: Abstraction and Reasoning Corpus</p>\n<p>Fran\u00e7ois Chollet</p>\n<p>https://arxiv.org/abs/1911.01547</p>\n<p><br></p>\n<p>[00:24:20] Systematic Generalization in Neural Nets</p>\n<p>Lake &amp; Baroni, 2023</p>\n<p>https://www.nature.com/articles/s41586-023-06668-3</p>\n<p><br></p>\n<p>[00:27:40] Open-Endedness &amp; Creativity in AI</p>\n<p>Tim Rockt\u00e4schel</p>\n<p>https://arxiv.org/html/2406.04268v1</p>\n<p><br></p>\n<p>[00:30:50] Fodor &amp; Pylyshyn on Connectionism</p>\n<p>https://www.sciencedirect.com/science/article/abs/pii/0010027788900315</p>\n<p><br></p>\n<p>[00:31:30] Tensor Product Representations</p>\n<p>Smolensky, 1990</p>\n<p>https://www.sciencedirect.com/science/article/abs/pii/000437029090007M</p>\n<p><br></p>\n<p>[00:35:50] DreamCoder: Wake-Sleep Program Synthesis</p>\n<p>Kevin Ellis et al.</p>\n<p>https://courses.cs.washington.edu/courses/cse599j1/22sp/papers/dreamcoder.pdf</p>\n<p><br></p>\n<p>[00:36:30] Compositional Generalization Benchmarks</p>\n<p>Ruis, Lake et al., 2022</p>\n<p>https://arxiv.org/pdf/2202.10745</p>\n<p><br></p>\n<p>[00:40:30] RNNs &amp; Tensor Products</p>\n<p>McCoy et al., 2018</p>\n<p>https://arxiv.org/abs/1812.08718</p>\n<p><br></p>\n<p>[00:46:10] Formal Causal Definition of Agency</p>\n<p>Kenton et al.</p>\n<p>https://arxiv.org/pdf/2208.08345v2</p>\n<p><br></p>\n<p>[00:48:40] Agency in Language Models</p>\n<p>Sumers et al.</p>\n<p>https://arxiv.org/abs/2309.02427</p>\n<p><br></p>\n<p>[00:55:20] Heider &amp; Simmel\u2019s Moving Shapes Experiment</p>\n<p>https://www.nature.com/articles/s41598-024-65532-0</p>\n<p><br></p>\n<p>[01:00:40] Language Models as Agent Models</p>\n<p>Jacob Andreas, 2022</p>\n<p>https://arxiv.org/abs/2212.01681</p>\n<p><br></p>\n<p>[01:13:35] Pragmatic Understanding in LLMs</p>\n<p>Ruis et al.</p>\n<p>https://arxiv.org/abs/2210.14986</p>\n<p><br></p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/97281129/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-20%2F808da284-ab46-0ae7-eb82-bb0885ebbcb1.mp3"
}