{
    "Id": "78-5A-5B-19-71-F5-E9-C2-32-5F-12-6B-7C-64-AD-74-1D-37-D3-B3-82-F3-A6-BD-F6-9E-27-8A-82-12-BD-81",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "Daniel Franzen & Jan Disselhoff - ARC Prize 2024 winners",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Daniel-Franzen--Jan-Disselhoff---ARC-Prize-2024-winners-e2upr5k",
    "Description": "<p>Daniel Franzen and Jan Disselhoff, the &quot;ARChitects&quot; are the official winners of the ARC Prize 2024. Filmed at Tufa Labs in Zurich - they revealed how they achieved a remarkable 53.5% accuracy by creatively utilising large language models (LLMs) in new ways. Discover their innovative techniques, including depth-first search for token selection, test-time training, and a novel augmentation-based validation system. Their results were extremely surprising.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!</p><p>https://centml.ai/pricing/</p><p><br></p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich.</p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>Jan Disselhoff</p><p>https://www.linkedin.com/in/jan-disselhoff-1423a2240/</p><p><br></p><p>Daniel Franzen</p><p>https://github.com/da-fr</p><p><br></p><p>ARC Prize: http://arcprize.org/</p><p><br></p><p>TRANSCRIPT AND BACKGROUND READING:</p><p>https://www.dropbox.com/scl/fi/utkn2i1ma79fn6an4yvjw/ARCHitects.pdf?rlkey=67pe38mtss7oyhjk2ad0d2aza&amp;dl=0</p><p><br></p><p>TOC</p><p>1. Solution Architecture and Strategy Overview</p><p>[00:00:00] 1.1 Initial Solution Overview and Model Architecture</p><p>[00:04:25] 1.2 LLM Capabilities and Dataset Approach</p><p>[00:10:51] 1.3 Test-Time Training and Data Augmentation Strategies</p><p>[00:14:08] 1.4 Sampling Methods and Search Implementation</p><p>[00:17:52] 1.5 ARC vs Language Model Context Comparison</p><p><br></p><p>2. LLM Search and Model Implementation</p><p>[00:21:53] 2.1 LLM-Guided Search Approaches and Solution Validation</p><p>[00:27:04] 2.2 Symmetry Augmentation and Model Architecture</p><p>[00:30:11] 2.3 Model Intelligence Characteristics and Performance</p><p>[00:37:23] 2.4 Tokenization and Numerical Processing Challenges</p><p><br></p><p>3. Advanced Training and Optimization</p><p>[00:45:15] 3.1 DFS Token Selection and Probability Thresholds</p><p>[00:49:41] 3.2 Model Size and Fine-tuning Performance Trade-offs</p><p>[00:53:07] 3.3 LoRA Implementation and Catastrophic Forgetting Prevention</p><p>[00:56:10] 3.4 Training Infrastructure and Optimization Experiments</p><p>[01:02:34] 3.5 Search Tree Analysis and Entropy Distribution Patterns</p><p><br></p><p>REFS</p><p>[00:01:05] Winning ARC 2024 solution using 12B param model, Franzen, Disselhoff, Hartmann</p><p>https://github.com/da-fr/arc-prize-2024/blob/main/the_architects.pdf</p><p><br></p><p>[00:03:40] Robustness of analogical reasoning in LLMs, Melanie Mitchell</p><p>https://arxiv.org/html/2411.14215</p><p><br></p><p>[00:07:50] Re-ARC dataset generator for ARC task variations, Michael Hodel</p><p>https://github.com/michaelhodel/re-arc</p><p><br></p><p>[00:15:00] Analysis of search methods in LLMs (greedy, beam, DFS), Chen et al.</p><p>https://arxiv.org/html/2408.00724v2</p><p><br></p><p>[00:16:55] Language model reachability space exploration, University of Toronto</p><p>https://www.youtube.com/watch?v=Bpgloy1dDn0</p><p><br></p><p>[00:22:30] GPT-4 guided code solutions for ARC tasks, Ryan Greenblatt</p><p>https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt</p><p><br></p><p>[00:41:20] GPT tokenization approach for numbers, OpenAI</p><p>https://platform.openai.com/docs/guides/text-generation/tokenizer-examples</p><p><br></p><p>[00:46:25] DFS in AI search strategies, Russell &amp; Norvig</p><p>https://www.amazon.com/Artificial-Intelligence-Modern-Approach-4th/dp/0134610997</p><p><br></p><p>[00:53:10] Paper on catastrophic forgetting in neural networks, Kirkpatrick et al.</p><p>https://www.pnas.org/doi/10.1073/pnas.1611835114</p><p><br></p><p>[00:54:00] LoRA for efficient fine-tuning of LLMs, Hu et al.</p><p>https://arxiv.org/abs/2106.09685</p><p><br></p><p>[00:57:20] NVIDIA H100 Tensor Core GPU specs, NVIDIA</p><p>https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/</p><p><br></p><p>[01:04:55] Original MCTS in computer Go, Yifan Jin</p><p>https://stanford.edu/~rezab/classes/cme323/S15/projects/montecarlo_search_tree_report.pdf</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/98413172/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-12%2Ff8b7b2e7-5a61-710b-2650-1958e0115cec.mp3"
}