{
    "Id": "A1-AE-49-09-4A-F2-8B-37-E0-EE-86-25-74-9D-E0-BC-20-B5-75-4B-D0-87-56-C3-D7-E9-FE-7B-9D-E9-8B-1A",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "#77 - Vitaliy Chiley (Cerebras)",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/77---Vitaliy-Chiley-Cerebras-e1k1hvu",
    "Description": "<p>Vitaliy Chiley &nbsp;is a Machine Learning Research Engineer at the next-generation computing hardware company Cerebras Systems. We spoke about how DL workloads including sparse workloads can run faster on Cerebras hardware.</p>\n<p><br></p>\n<p>[00:00:00] Housekeeping</p>\n<p>[00:01:08] Preamble</p>\n<p>[00:01:50] Vitaliy Chiley Introduction</p>\n<p>[00:03:11] Cerebrus architecture</p>\n<p>[00:08:12] Memory management and FLOP utilisation</p>\n<p>[00:18:01] Centralised vs decentralised compute architecture</p>\n<p>[00:21:12] Sparsity</p>\n<p>[00:23:47] Does Sparse NN imply Heterogeneous compute?</p>\n<p>[00:29:21] Cost of distributed memory stores?</p>\n<p>[00:31:01] Activation vs weight sparsity</p>\n<p>[00:37:52] What constitutes a dead weight to be pruned?</p>\n<p>[00:39:02] Is it still a saving if we have to choose between weight and activation sparsity?</p>\n<p>[00:41:02] Cerebras is a cool place to work</p>\n<p>[00:44:05] What is sparsity? Why do we need to start dense?&nbsp;</p>\n<p>[00:46:36] Evolutionary algorithms on Cerebras?</p>\n<p>[00:47:57] How can we start sparse? Google RIGL</p>\n<p>[00:51:44] Inductive priors, why do we need them if we can start sparse?</p>\n<p>[00:56:02] Why anthropomorphise inductive priors?</p>\n<p>[01:02:13] Could Cerebras run a cyclic computational graph?</p>\n<p>[01:03:16] Are NNs locality sensitive hashing tables?</p>\n<p><br></p>\n<p>References;</p>\n<p>Rigging the Lottery: Making All Tickets Winners [RIGL]</p>\n<p>https://arxiv.org/pdf/1911.11134.pdf</p>\n<p><br></p>\n<p>[D] DanNet, the CUDA CNN of Dan Ciresan in Jurgen Schmidhuber's team, won 4 image recognition challenges prior to AlexNet</p>\n<p>https://www.reddit.com/r/MachineLearning/comments/dwnuwh/d_dannet_the_cuda_cnn_of_dan_ciresan_in_jurgen/&nbsp;</p>\n<p><br></p>\n<p>A Spline Theory of Deep Learning [Balestriero]</p>\n<p>https://proceedings.mlr.press/v80/balestriero18b.html&nbsp;</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/53577150/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-5-16%2F657fdf4b-2e57-c09b-67c2-826794d27185.mp3"
}