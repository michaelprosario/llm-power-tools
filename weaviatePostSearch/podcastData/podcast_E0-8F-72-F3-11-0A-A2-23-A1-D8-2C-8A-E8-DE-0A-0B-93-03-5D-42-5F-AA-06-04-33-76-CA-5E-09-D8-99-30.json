{
    "Id": "E0-8F-72-F3-11-0A-A2-23-A1-D8-2C-8A-E8-DE-0A-0B-93-03-5D-42-5F-AA-06-04-33-76-CA-5E-09-D8-99-30",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "Reasoning, Robustness, and Human Feedback in AI - Max Bartolo (Cohere)",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Reasoning--Robustness--and-Human-Feedback-in-AI---Max-Bartolo-Cohere-e30c1uu",
    "Description": "<p>Dr. Max Bartolo from Cohere discusses machine learning model development, evaluation, and robustness. Key topics include model reasoning, the DynaBench platform for dynamic benchmarking, data-centric AI development, model training challenges, and the limitations of human feedback mechanisms. The conversation also covers technical aspects like influence functions, model quantization, and the PRISM project.</p><p><br></p><p>Max Bartolo (Cohere):</p><p>https://www.maxbartolo.com/</p><p>https://cohere.com/command</p><p><br></p><p>TRANSCRIPT:</p><p>https://www.dropbox.com/scl/fi/vujxscaffw37pqgb6hpie/MAXB.pdf?rlkey=0oqjxs5u49eqa2m7uaol64lbw&amp;dl=0</p><p><br></p><p>TOC:</p><p>1. Model Reasoning and Verification</p><p> [00:00:00] 1.1 Model Consistency and Reasoning Verification</p><p>   [00:03:25] 1.2 Influence Functions and Distributed Knowledge Analysis</p><p>   [00:10:28] 1.3 AI Application Development and Model Deployment</p><p>   [00:14:24] 1.4 AI Alignment and Human Feedback Limitations</p><p><br></p><p>2. Evaluation and Bias Assessment</p><p>   [00:20:15] 2.1 Human Evaluation Challenges and Factuality Assessment</p><p>   [00:27:15] 2.2 Cultural and Demographic Influences on Model Behavior</p><p>   [00:32:43] 2.3 Adversarial Examples and Model Robustness</p><p><br></p><p>3. Benchmarking Systems and Methods</p><p>   [00:41:54] 3.1 DynaBench and Dynamic Benchmarking Approaches</p><p>   [00:50:02] 3.2 Benchmarking Challenges and Alternative Metrics</p><p>   [00:50:33] 3.3 Evolution of Model Benchmarking Methods</p><p>   [00:51:15] 3.4 Hierarchical Capability Testing Framework</p><p>   [00:52:35] 3.5 Benchmark Platforms and Tools</p><p><br></p><p>4. Model Architecture and Performance</p><p>   [00:55:15] 4.1 Cohere&#39;s Model Development Process</p><p>   [01:00:26] 4.2 Model Quantization and Performance Evaluation</p><p>   [01:05:18] 4.3 Reasoning Capabilities and Benchmark Standards</p><p>   [01:08:27] 4.4 Training Progression and Technical Challenges</p><p><br></p><p>5. Future Directions and Challenges</p><p>   [01:13:48] 5.1 Context Window Evolution and Trade-offs</p><p>   [01:22:47] 5.2 Enterprise Applications and Future Challenges</p><p><br></p><p>REFS:</p><p>[00:03:10] Research at Cohere with Laura Ruis et al., Max Bartolo, Laura Ruis et al.</p><p>https://cohere.com/research/papers/procedural-knowledge-in-pretraining-drives-reasoning-in-large-language-models-2024-11-20</p><p>[00:04:15] Influence functions in machine learning, Koh &amp; Liang</p><p>https://arxiv.org/abs/1703.04730</p><p>[00:08:05] Studying Large Language Model Generalization with Influence Functions, Roger Grosse et al.</p><p>https://storage.prod.researchhub.com/uploads/papers/2023/08/08/2308.03296.pdf</p><p>[00:11:10] The LLM ARChitect: Solving ARC-AGI Is A Matter of Perspective, Daniel Franzen, Jan Disselhoff, and David Hartmann</p><p>https://github.com/da-fr/arc-prize-2024/blob/main/the_architects.pdf</p><p>[00:12:10] Hugging Face model repo for C4AI Command A, Cohere and Cohere For AI</p><p>https://huggingface.co/CohereForAI/c4ai-command-a-03-2025</p><p>[00:13:30] OpenInterpreter</p><p>https://github.com/KillianLucas/open-interpreter</p><p>[00:16:15] Human Feedback is not Gold Standard, Tom Hosking, Max Bartolo, Phil Blunsom</p><p>https://arxiv.org/abs/2309.16349</p><p>[00:27:15] The PRISM Alignment Dataset, Hannah Kirk et al.</p><p>https://arxiv.org/abs/2404.16019</p><p>[00:32:50] How adversarial examples arise, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry</p><p>https://arxiv.org/abs/1905.02175</p><p>[00:43:00] DynaBench platform paper, Douwe Kiela et al.</p><p>https://aclanthology.org/2021.naacl-main.324.pdf</p><p>[00:50:15] Sara Hooker&#39;s work on compute limitations, Sara Hooker</p><p>https://arxiv.org/html/2407.05694v1</p><p>[00:53:25] DataPerf: Community-led benchmark suite, Mazumder et al.</p><p>https://arxiv.org/abs/2207.10062</p><p>[01:04:35] DROP, Dheeru Dua et al.</p><p>https://arxiv.org/abs/1903.00161</p><p>[01:07:05] GSM8k, Cobbe et al.</p><p>https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k</p><p>[01:09:30] ARC, Fran\u00e7ois Chollet</p><p>https://github.com/fchollet/ARC-AGI</p><p>[01:15:50] Command A, Cohere</p><p>https://cohere.com/blog/command-a</p><p>[01:22:55] Enterprise search using LLMs, Cohere</p><p>https://cohere.com/blog/commonly-asked-questions-about-search-from-coheres-enterprise-customers</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/100058526/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-18%2F936d6a62-ffe6-effb-7f41-e798337ea80c.mp3"
}