{
    "Id": "B3-7B-30-4F-4C-B9-B9-96-77-32-9F-E1-1F-83-EB-6E-B4-0F-31-20-27-FD-D5-21-21-98-CE-58-90-0E-E0-AA",
    "ContentSourceId": "425b36ab-3b93-42c9-8cbc-032e3bddb1d6",
    "Title": "Moving NLP Forward With Transformer Models and Attention",
    "SourceUrl": "https://realpython.com/podcasts/rpp/121/",
    "Description": "<p>What&rsquo;s the big breakthrough for Natural Language Processing (NLP) that has dramatically advanced machine learning into deep learning? What makes these transformer models unique, and what defines &ldquo;attention?&rdquo; This week on the show, Jodie Burchell, developer advocate for data science at JetBrains, continues our talk about how machine learning (ML) models understand and generate text.</p>\n<p>This episode is a continuation of the conversation in episode #119. Jodie builds on the concepts of bag-of-words, word2vec, and simple embedding models. We talk about the breakthrough mechanism called &ldquo;attention,&rdquo; which allows for parallelization in building models. </p>\n<p>We also discuss the two major transformer models, BERT and GPT3. Jodie continues to share multiple resources to help you continue exploring modeling and NLP with Python.</p>\n<div class=\"alert alert-primary\" role=\"alert\">\n<p><strong>Course Spotlight:</strong> <a href=\"https://realpython.com/courses/build-neural-network-python-ai/\">Building a Neural Network &amp; Making Predictions With Python AI</a></p>\n<p>In this step-by-step course, you&rsquo;ll build a neural network from scratch as an introduction to the world of artificial intelligence (AI) in Python. You&rsquo;ll learn how to train your neural network and make predictions based on a given dataset.</p>\n</div>\n<p>Topics:</p>\n<ul>\n<li>00:00:00 &ndash; Introduction</li>\n<li>00:02:20 &ndash; Where we left off with word2vec&hellip;</li>\n<li>00:03:35 &ndash; Example of losing context</li>\n<li>00:06:50 &ndash; Working at scale and adding attention</li>\n<li>00:12:34 &ndash; Multiple levels of training for the model </li>\n<li>00:14:10 &ndash; Attention is the basis for transformer models</li>\n<li>00:15:07 &ndash; BERT (Bidirectional Encoder Representations from Transformers)</li>\n<li>00:16:29 &ndash; GPT (Generative Pre-trained Transformer)</li>\n<li>00:19:08 &ndash; Video Course Spotlight</li>\n<li>00:20:08 &ndash; How far have we moved forward?</li>\n<li>00:20:41 &ndash; Access to GPT-2 via Hugging Face</li>\n<li>00:23:56 &ndash; How to access and use these models?</li>\n<li>00:30:42 &ndash; Cost of training GPT-3</li>\n<li>00:35:01 &ndash; Resources to practice and learn with BERT</li>\n<li>00:38:19 &ndash; GPT-3 and GitHub Copilot</li>\n<li>00:44:35 &ndash; DALL-E is a transformer</li>\n<li>00:46:13 &ndash; Help yourself to the show notes!</li>\n<li>00:49:19 &ndash; How can people follow your work?</li>\n<li>00:50:03 &ndash; Thanks and goodbye</li>\n</ul>\n<p>Show Links:</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Recurrent_neural_network\">Recurrent neural network - Wikipedia</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Long_short-term_memory\">Long short-term memory - Wikipedia</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Vanishing_gradient_problem\">Vanishing gradient problem - Wikipedia</a></li>\n<li><a href=\"https://www.mygreatlearning.com/blog/the-vanishing-gradient-problem/\">Vanishing Gradient Problem | What is Vanishing Gradient Problem?</a></li>\n<li><a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need | Cornell University</a></li>\n<li><a href=\"https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) \u2013 Jay Alammar</a></li>\n<li><a href=\"https://arxiv.org/abs/2204.10019\">Standing on the Shoulders of Giant Frozen Language Models | Cornell University</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=NKAK8ttzd6Q\">#datalift22 Embeddings paradigm shift: Model training to vector similarity search by Nava Levy - YouTube</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=TQQlZhbC5ps&amp;list=LL&amp;index=4\">Transformer Neural Networks - EXPLAINED! (Attention is all you need) - YouTube</a></li>\n<li><a href=\"https://huggingface.co/blog/bert-101\">BERT 101 - State Of The Art NLP Model Explained</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=MQnJZuBGmSQ\">How GPT3 Works - Easily Explained with Animations - YouTube</a></li>\n<li><a href=\"https://transformer.huggingface.co/doc/gpt2-large\">Write With Transformer (GPT2 Live Playground Tool) - Hugging Face</a></li>\n<li><a href=\"https://opt.alpa.ai/\">Language Model with Alpa (GPT3 Live Playground Tool) OPT-175B</a></li>\n<li><a href=\"https://www.bigdata.fm/\">Big Data | Music</a></li>\n<li><a href=\"https://beta.openai.com/playground\">OpenAI API</a></li>\n<li><a href=\"https://huggingface.co/docs/transformers/notebooks\">\ud83e\udd17 (Hugging Face)Transformers Notebooks</a></li>\n<li><a href=\"https://realpython.com/github-copilot-python/\">GitHub Copilot: Fly With Python at the Speed of Thought</a></li>\n<li><a href=\"https://twitter.com/sotak/status/1546452033816985601\">GitHub Copilot learned about the daily struggle of JavaScript developers after being trained on billions of lines of code. | Marek Sotak on Twitter</a></li>\n<li><a href=\"https://t-redactyl.io/\">Jodie Burchell&rsquo;s Blog - Standard error</a></li>\n<li><a href=\"https://twitter.com/t_redactyl\">Jodie Burchell \ud83c\udde6\ud83c\uddfa\ud83c\udde9\ud83c\uddea (@t_redactyl) / Twitter</a></li>\n<li><a href=\"https://www.jetbrains.com/\">JetBrains: Essential tools for software developers and teams</a></li>\n</ul>\n<p>Level up your Python skills with our expert-led courses:</p>\n<ul>\n<li><a href=\"https://realpython.com/courses/build-neural-network-python-ai/\">Building a Neural Network &amp; Making Predictions With Python AI</a></li>\n<li><a href=\"https://realpython.com/courses/text-classification-with-keras/\">Learn Text Classification With Python and Keras</a></li>\n<li><a href=\"https://realpython.com/courses/pandas-dataframe-working-with-data/\">The pandas DataFrame: Working With Data Efficiently</a></li>\n</ul> <p><a rel=\"payment\" href=\"https://realpython.com/join\">Support the podcast &amp; join our community of Pythonistas</a></p>",
    "EnclosureUrl": "https://dts.podtrac.com/redirect.mp3/files.realpython.com/podcasts/RPP_E121_Jodie.c8f1a95de5d0.mp3"
}