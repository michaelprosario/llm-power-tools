{
    "Id": "7B-93-05-40-D7-4A-37-A8-F0-A5-DD-67-6A-B1-8E-30-BF-45-02-5F-C8-A8-EE-EA-93-66-53-28-30-ED-0E-46",
    "ContentSourceId": "08ddc66c-88c1-4fd9-9d0f-06779ee4a5cb",
    "Title": "Test-Time Adaptation: the key to reasoning with DL (Mohamed Osman)",
    "SourceUrl": "https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Test-Time-Adaptation-the-key-to-reasoning-with-DL-Mohamed-Osman-e30hd8t",
    "Description": "<p>Mohamed Osman joins to discuss MindsAI&#39;s highest scoring entry to the ARC challenge 2024 and the paradigm of test-time fine-tuning. They explore how the team, now part of Tufa Labs in Zurich, achieved state-of-the-art results using a combination of pre-training techniques, a unique meta-learning strategy, and an ensemble voting mechanism. Mohamed emphasizes the importance of raw data input and flexibility of the network.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>TRANSCRIPT + REFS:</p><p>https://www.dropbox.com/scl/fi/jeavyqidsjzjgjgd7ns7h/MoFInal.pdf?rlkey=cjjmo7rgtenxrr3b46nk6yq2e&amp;dl=0</p><p><br></p><p>Mohamed Osman (Tufa Labs)</p><p>https://x.com/MohamedOsmanML</p><p><br></p><p>Jack Cole (Tufa Labs)</p><p>https://x.com/MindsAI_Jack</p><p><br></p><p>How and why deep learning for ARC paper:</p><p>https://github.com/MohamedOsman1998/deep-learning-for-arc/blob/main/deep_learning_for_arc.pdf</p><p><br></p><p>TOC:</p><p>1. Abstract Reasoning Foundations</p><p> [00:00:00] 1.1 Test-Time Fine-Tuning and ARC Challenge Overview</p><p>   [00:10:20] 1.2 Neural Networks vs Programmatic Approaches to Reasoning</p><p>   [00:13:23] 1.3 Code-Based Learning and Meta-Model Architecture</p><p>   [00:20:26] 1.4 Technical Implementation with Long T5 Model</p><p><br></p><p>2. ARC Solution Architectures</p><p>   [00:24:10] 2.1 Test-Time Tuning and Voting Methods for ARC Solutions</p><p>   [00:27:54] 2.2 Model Generalization and Function Generation Challenges</p><p>   [00:32:53] 2.3 Input Representation and VLM Limitations</p><p>   [00:36:21] 2.4 Architecture Innovation and Cross-Modal Integration</p><p>   [00:40:05] 2.5 Future of ARC Challenge and Program Synthesis Approaches</p><p><br></p><p>3. Advanced Systems Integration</p><p>   [00:43:00] 3.1 DreamCoder Evolution and LLM Integration</p><p>   [00:50:07] 3.2 MindsAI Team Progress and Acquisition by Tufa Labs</p><p>   [00:54:15] 3.3 ARC v2 Development and Performance Scaling</p><p>   [00:58:22] 3.4 Intelligence Benchmarks and Transformer Limitations</p><p>   [01:01:50] 3.5 Neural Architecture Optimization and Processing Distribution</p><p><br></p><p>REFS:</p><p>[00:01:32] Original ARC challenge paper, Fran\u00e7ois Chollet</p><p>https://arxiv.org/abs/1911.01547</p><p><br></p><p>[00:06:55] DreamCoder, Kevin Ellis et al.</p><p>https://arxiv.org/abs/2006.08381</p><p><br></p><p>[00:12:50] Deep Learning with Python, Fran\u00e7ois Chollet</p><p>https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438</p><p><br></p><p>[00:13:35] Deep Learning with Python, Fran\u00e7ois Chollet</p><p>https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438</p><p><br></p><p>[00:13:35] Influence of pretraining data for reasoning, Laura Ruis</p><p>https://arxiv.org/abs/2411.12580</p><p><br></p><p>[00:17:50] Latent Program Networks, Clement Bonnet</p><p>https://arxiv.org/html/2411.08706v1</p><p><br></p><p>[00:20:50] T5, Colin Raffel et al.</p><p>https://arxiv.org/abs/1910.10683</p><p><br></p><p>[00:30:30] Combining Induction and Transduction for Abstract Reasoning, Wen-Ding Li, Kevin Ellis et al.</p><p>https://arxiv.org/abs/2411.02272</p><p><br></p><p>[00:34:15] Six finger problem, Chen et al.</p><p>https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_SpatialVLM_Endowing_Vision-Language_Models_with_Spatial_Reasoning_Capabilities_CVPR_2024_paper.pdf</p><p><br></p><p>[00:38:15] DeepSeek-R1-Distill-Llama, DeepSeek AI</p><p>https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B</p><p><br></p><p>[00:40:10] ARC Prize 2024 Technical Report, Fran\u00e7ois Chollet et al.</p><p>https://arxiv.org/html/2412.04604v2</p><p><br></p><p>[00:45:20] LLM-Guided Compositional Program Synthesis, Wen-Ding Li and Kevin Ellis</p><p>https://arxiv.org/html/2503.15540</p><p><br></p><p>[00:54:25] Abstraction and Reasoning Corpus, Fran\u00e7ois Chollet</p><p>https://github.com/fchollet/ARC-AGI</p><p><br></p><p>[00:57:10] O3 breakthrough on ARC-AGI, OpenAI</p><p>https://arcprize.org/</p><p><br></p><p>[00:59:35] ConceptARC Benchmark, Arseny Moskvichev, Melanie Mitchell</p><p>https://arxiv.org/abs/2305.07141</p><p><br></p><p>[01:02:05] Mixtape: Breaking the Softmax Bottleneck Efficiently, Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W.</p><p>http://papers.neurips.cc/paper/9723-mixtape-breaking-the-softmax-bottleneck-efficiently.pdf</p>\n",
    "EnclosureUrl": "https://anchor.fm/s/1e4a0eac/podcast/play/100233949/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-22%2Fed4795dc-93de-d0ab-94b3-c2face88c2fd.mp3"
}